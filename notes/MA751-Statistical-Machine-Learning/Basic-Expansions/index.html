
        <!DOCTYPE html>
        <html>
            <head>
                <meta http-equiv="Content-Type" content="text/html; charset=gb2312">
                <title>Basic Expansions - Jimmy Z. Hu's Blog</title>
                <style rel="stylesheet" type="text/css">
                    body {font-family: Georgia; font-size: 17px;}
h2 {margin-top: 40px; border-bottom: 1px solid #999; text-transform: uppercase;}
p {text-align: left; line-height: 1.4em;}
pre {
    background: #f4f4f4; border: 1px solid #ddd; border-left: 3px solid #9f9f9f; 
    color: #666; page-break-inside: avoid; font-family: monospace; font-size: 15px; 
    line-height: 1.1; max-width: 100%; overflow: auto; padding: 0.5em 0.5em; 
    display: block; word-wrap: break-word;
}
li {line-height: 1.25em;}
a:link {text-decoration: none; color: blue;}
a:visited {text-decoration: none; color: blue;}
.container {width: 750px; margin-left: auto; margin-right: auto; padding: 50px 0 50px 0;}
.title {text-align: left;}
.katex {font-size: 1em !important;} 
table {width: 100%%;}
table, th, td {border: 1px solid black;}

                </style>
                <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
                <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
                <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
                <script>
                    document.addEventListener("DOMContentLoaded", function() {
                        renderMathInElement(document.body, {
                            delimiters: [
                              {left: "$$", right: "$$", display: true},
                              {left: "$", right: "$", display: false}
                            ] 
                        });
                    });
                </script>
            </head>
            <body>
                <div class="container">
                    <h1 class="title">Basic Expansions</h1>
                    2020-03-01
                    <span style="float:right; _position:relative;">
                        <a href="/home">HOME</a>
                        &ensp;
                        <a href="/notes">NOTES</a> &ensp; <a href="../">RELATED</a>
                    </span><br/>
                    <h2>NOTES 02/25</h2>
<h3>Basis Expansions</h3>
<p>We want to get away from the convenient but restrictive linearity assumptions by expending the model and adding transformed feature.</p>
<p>Suppose we define $M$ transformations. Originally, we have</p>
<p>$$h_m: \mathbb{R}^p \xrightarrow{f} \mathbb{R}, \space f(\mathbf{X}) = \mathbf{X}^{\top} \beta = \sum_{i=1}^{p} \mathbf{x}_i^{\top} \beta$$</p>
<p>But now we have</p>
<p>$$h_m: \mathbb{R}^p \xrightarrow{f} \mathbb{R}, \space f(\mathbf{X}) = \sum_{m=1}^{M} h_m(\mathbf{X}) \beta_M$$</p>
<p>In general, we define a dictionary $\mathcal{D}$ consisting of typically a very large number $|\mathcal{D}|$ of basis functions.</p>
<ul>
<li>$h_m(\mathbf{X}) = \prod_{j=1}{p} \mathbf{x}_j^{d_{mj}}$, where $\sum_{j=1}^{p}d_{mj} = d_{m}$
<ul>
<li>$h_m(\mathbf{X}) = x_j$， $h_m(\mathbf{X}) = x_k^2$, $h_m(\mathbf{X}) = x_jx_k$</li>
</ul>
</li>
<li>$h_m(\mathbf{X}) = \frac{x_j^{\lambda} - 1}{\lambda}$, $h_m(\mathbf{X}) \xrightarrow{\lambda \rightarrow 0} \log x_j$</li>
<li>$h_m(\mathbf{X}) = \mathrm{I}\left[ L_m \leq x_m &lt; U_m \right]$</li>
</ul>
<h3>Piecewise Polynomials and Splines</h3>
<p>A more general family of transformations involves piecewise polynomials. We start by segmenting the domain of $\mathbf{X}$ according to &quot;knots&quot; $\xi_1$, $\xi_2$, ..., $\xi_k$. Then we have</p>
<p>$$f(x) = \sum_{j=1}^{k+1} \sum_{m=1}^{M} \beta_{jm} x^{m-1} \mathrm{I} [\xi_{j-1} \leq x &lt; \xi_j],\space \xi_0 =- \infty,\space \xi_{k+1} = \infty$$</p>
<ul>
<li>
<p>Example 1: $k=2$, $M=1$. Thus, $f(x) = \sum_{j=1}^{k=3} \beta_j h_j(x)$.
And, $h_1 = \mathrm{I} [x &lt; \xi_1]$, $h_2 = \mathrm{I} [\xi_1 \leq x &lt; \xi_2]$, $h_3 = \mathrm{I} [\xi_2 \leq x]$</p>
</li>
<li>
<p>Example 2: $k=2$, $M=2$. Thus, $f(x) = \sum_{j=1}^{k=3} h_j(x) (\beta_j + \beta_{j+k+1}x)$.</p>
</li>
</ul>
<center><img src="./spline1.svg" width='50%'></center>
<p>We can further require that $f$ is continious at knots</p>
<p> $f(\xi_{1}^{-}) = f(\xi_{1}^{+}) \Rightarrow \beta_1 + \beta_4 \xi_1 = \beta_2 + \beta_5 \xi_1$
 $f(\xi_{2}^{-}) = f(\xi_{2}^{+}) \Rightarrow \beta_2 + \beta_5 \xi_1 = \beta_3 + \beta_6 \xi_1$</p>
<p><u>Note</u>: $(x - \xi)_{+} = (x - \xi) \mathrm{I}[x \geq \xi]$</p>
<p>This model is equivalent to:</p>
<p>$h_1(x) = 1$, $h_2(x) = x$, $h_3(x) = (x - \xi_{1})_{+}$, $h_1(x) = (x - \xi_{2})_{+}$</p>
<p>$$f(x) = \begin{cases} \beta_1 + \beta_2 x, &amp; x &lt; \xi_1  \\ \beta_1 + \beta_2 x + \beta_3 (x - \xi_1)_{+} &amp; \xi_1 \leq x &lt; \xi_2 \\ \beta_1 + \beta_2 x + \beta_3 (x - \xi_1)_{+} + \beta_4 (x - \xi_2)_{+}, &amp; \xi_2 \leq x \end{cases} $$</p>
<p><u>Proof:</u></p>
<p>Consider initially a single knot $\xi$ and the model specified by two piecewise linear functions</p>
<p>$$f(X)=\left(\beta_{1}+\beta_{2} X\right) I[X&lt;\xi]+\left(\gamma_{1}+\gamma_{2} X\right) I[X \geq \xi]$$</p>
<p>Since $I[X &lt; \xi] = \left(1 - I[X \geq \xi]\right)$, we get</p>
<p>$$f(X) = \beta_1 + \beta_2 X + (\gamma_1 - \beta_1) I[X \geq \xi] + (\gamma_2 - \beta_2) X I[X \geq \xi]$$</p>
<p>Considering the ristriction $\beta_1 + \beta_2 \xi = \gamma_1 + \gamma_2 \xi \Rightarrow \gamma_1 - \beta_1 = (\beta_2 - \gamma_2) \xi$, we have</p>
<p>$$f(X) = \beta_1 + \beta_2 X + (\gamma_2 - \beta_2) (X - \xi) I[X \geq \xi] = \beta_1 + \beta_2 X + \beta_3 (X - \xi)_{+}$$</p>
<p>And $\sum_{j=1}^{2K+2} \gamma_j g_j(X) = \sum_{j=1}^{K+1} (\gamma_j + \gamma_{j+K+1} X) I[\xi_{j-1} \leq X &lt; \xi_{j}]$</p>
<p>Suppose now that we have $K$ knots $\xi_{1}, \ldots, \xi_{K}$ and consider a piecewise linear $f:$ for $j=1, \ldots, K+1,$ with $g_{j}(X)=I\left[\xi_{j-1} \leq X&lt;\xi_{j}\right]$ and $g_{j+K+1}(X)=X g_{j}(X)$, $f(X)=\sum_{j=1}^{2 K+2} \gamma_{j} g_{j}(X) .$ Show then, using induction, that constraining $f$ to be continuous at each knot is equivalent to the linear spline $f(X)=\sum_{j=1}^{K+2} \beta_{j} h_{j}(X)$ where</p>
<p>$h_{1}(X)=1, \quad h_{2}(X)=X$, and $h_{j+2}(X)=\left(X-\xi_{j}\right)_{+},$ for $j=1, \ldots, K$</p>
<p>Assume that for a certain $K, K \in \mathbb{R}^{+}$, we have</p>
<p>$$\sum_{j=1}^{2K+2} \gamma_j g_j(X) = \sum_{j=1}^{K+2} \beta_j h_j (X)$$</p>
<p>Consider $K+1$</p>
<p>$$\begin{aligned} &amp;\sum_{j=1}^{2(K+1)+2} \gamma_j g_j(X) - \sum_{j=1}^{2K+2} \gamma_j g_j(X) \\ =&amp; \sum_{j=1}^{K+2} (\gamma_j + \gamma_{j+K+1} X) I[\xi_{j-1} \leq X &lt; \xi_{j}] - \sum_{j=1}^{K+1} (\gamma_j + \gamma_{j+K+1} X) I[\xi_{j-1} \leq X &lt; \xi_{j}] \\ =&amp; (\gamma_{K+2} +\gamma_{2K+3} X) I[\xi_{K+1} \leq X] + (\gamma_{K+1} + \gamma_{2K+2} X) I[\xi_{K} \leq X &lt; \xi_{K+1}] \\ &amp;- (\gamma_{K+1} + \gamma_{2K+2} X) I[\xi_{K} \leq X] \\ =&amp; (\gamma_{K+2} - \gamma_{K+1}) I[\xi_{K+1} \leq X] + (\gamma_{2K+3} - \gamma_{2K+2}) X I[\xi_{K+1} \leq X]\end{aligned}$$</p>
<p>Considering the ristriction $\gamma_{K+1} + \gamma_{2K+2} \xi_{K+1} = \gamma_{K+2} + \gamma_{2K+3} \xi_{K+1} \Rightarrow \gamma_{K+2} - \gamma_{K+1} = (\gamma_{2K+3} - \gamma_{2K+2}) \xi_{K+1}$, and let $\beta_{K+3} = \gamma_{2K+3} - \gamma_{2K+2}$. Then we will have</p>
<p>$$\begin{aligned} &amp;(\gamma_{K+2} - \gamma_{K+1}) I[\xi_{K+1} \leq X] + (\gamma_{2K+3} - \gamma_{2K+2}) X I[\xi_{K+1} \leq X] \\ =&amp; \beta_{K+2}(X-\xi_{K+1}) I[\xi_{K+1} \leq X] = \beta_{K+3}(X - \xi_{K+1})_{+} \end{aligned}$$</p>
<p>Now we have</p>
<p>$$\begin{aligned} \sum_{j=1}^{2(K+1)+2} \gamma_j g_j(X) &amp;= \sum_{j=1}^{2K+2} \gamma_j g_j(X) + \beta_{K+3}(X - \xi_{K+1})_{+} \\ &amp;= \sum_{j=1}^{K+2} \beta_j h_j (X) + \beta_{K+3}(X - \xi_{K+1})_{+} \\ &amp;= \sum_{j=1}^{(K+1)+2} \beta_j h_j (X) \end{aligned}$$</p>
<p>In general, we can use higher order polynomicals at each segment and require continity up to the derivative of order $M-1$.</p>
<p>The basis can be shown below:</p>
<p><strong>&quot;Spline&quot; (order $M$)</strong></p>
<p>In most cases, $M = 1,2,4$</p>
<p>$h_1(x) = 1$, $h_2(x) = x$, ..., $h_m(x) = x^{m-1}$</p>
<p>$h_{m+1}(x) = (x - \xi_1)_{+}^{M-1}$, $h_{m+2}(x) = (x - \xi_2)_{+}^{M-1}$, ..., $h_{m+k}(x) = (x - \xi_k)_{+}^{M-1}$</p>
<p>$f(x) = \sum_{m=1}^{M} \left[ \beta_m x^{m-1} + \sum_{j=1}^{k} \beta_{M+j} (x - \xi_j)_{+}^{M-1} \right]$</p>
<p>In prectice, we use</p>
<ul>
<li>B-Spline (Bayes Spline), which is equivalent to cubic splines but numericaly more stable.</li>
<li>Nature Spline: we add extra contrain that require the model to be linear. The regions are defined by the boundary knots.</li>
</ul>
<h3>Smoothing Splines</h3>
<p>Recap $f(x) = \sum_{h_m \in \mathbb{F}} h_m(x) \beta_m$</p>
<p>We want $\hat{\beta} = \underset{\beta}{\mathrm{argmax}} \space D(y, f(x)) + \mathrm{Penalty_{\space M, \xi}(\beta)}$</p>
<p>Alternatively, instead of selecting knots, we can use a &quot;smoothing spline&quot;. The idea is to just represent the dictionary using a suitable function space and</p>
<p>$$\hat{f} = \underset{f \in \mathbb{W}}{\mathrm{argmin}} \space \sum_{i=1}^{h} (y_i - f(x_i))^2 + \lambda \int \left[ f^{\prime \prime} (t) \right]^2 dt$$</p>
<p>Remarkably, it can be shown that $\hat{f}_{\lambda}$ is a natural cubic spline with knots at each observation.</p>
<p>In this case,</p>
<p>$$f(\mathbf{X}_i) = \sum_{m=1}^{n} N_m (\mathbf{x}_i) \theta_m = N_m (\mathbf{X}_i)^{\top} \theta_m$$</p>
<p>$$f = \left[f(\mathbf{X}_i)\right]_{i=1,\dots,n} = N(\mathbf{X}) \theta$$</p>
<p>Therefore, $\hat{f}$ is equivalent to</p>
<p>$$\hat{\theta}_{\lambda} = \underset{\theta}{\mathrm{argmin}} (\mathbf{y}-N \theta)^{\top} (\mathbf{y}-N \theta) + \lambda \theta^{\top} \Omega_{N} \theta \mathrm{,\space where} \space \Omega_{N} = \left[ \int N_j^{\prime \prime} (t) N_k^{\prime \prime} (t)dt \right]_{jk}$$</p>
<p>Thus,</p>
<p>$$\hat{\theta}_{\lambda} = (N^{\top} N + \lambda \Omega N)^{-1} N^{\top} y$$</p>
<p>$$\hat{f}_{\lambda} = N \hat{\theta}_{\lambda} = N (N^{\top} N + \lambda \Omega N)^{-1} N^{\top} y$$</p>
<p>We call $N \hat{\theta}_{\lambda} = N (N^{\top} N + \lambda \Omega N)^{-1} N^{\top}$ as &quot;smoother matrix&quot; $S_{\lambda}$. Define effective degree of freedom $df_{\lambda} = tr(S_{\lambda})$.</p>
<h2>NOTES 02/27</h2>
<h3>Reproducing kernal Hilbert space (RKHS)</h3>
<p>A general class of regularization has the form</p>
<p>$$\underset{f \in \mathcal{H}}{min} \sum_{i=1}^{n} D(y_i, f(x_i)) + \lambda J(f)$$</p>
<p>(e.g. $J(f) = \int \left[ f^{\prime \prime} (t) \right]^2 dt$)</p>
<p>$f \in \mathcal{H}$ means $J(f)&lt;0$</p>
<p>An interesting subclass of these problems arises when $\mathcal{H}$ is a reproducing kernal Hilbert space (RKHS). A Hilbert space is a complete matrix space possessing the structure of an inner product that allows length and angle to be measured: $\langle x, y \rangle _{\mathcal{H}},x, y \in \mathcal{H}$. We can define the norm $||x||_{\mathcal{H}}^2= \langle x,x \rangle _{\mathcal{H}}$, and the distance $d(x, y) = ||x-y||_{\mathcal{H}}$.</p>
<p>The evaluation is the function $L_x(f) = f(x)$. In RKHS, every evaluation is a continuous function by Riesz representation theorem</p>
<p>$$f(x) = L_x(f) = \langle k_x, f \rangle _{\mathcal{H}}, x \in \mathbb{R} \Leftrightarrow k_x \in \mathcal{H}$$</p>
<p>$$k_x(y) = L_y(k_x) = \langle k_x, k_y \rangle _{\mathcal{H}}$$</p>
<p>Thus, we can define $K(x,y) = \langle k_x, k_y \rangle _{\mathcal{H}} = K(y, x)$ that is symmetric.</p>
<p>The Moore–Aronszajn theorem states that every symmetric, positive definite kernel defines a unique reproducing kernel Hilbert space, that is, $K \Leftrightarrow \mathcal{H}_K$</p>
<p>(Reproducing property) Thus, for every $x \in \mathbb{R}, k_x = K(x, \cdot)$
we have $f(x) = \sum_{i=1}^{J} \alpha_i K(x, x_i) \in \mathcal{H}_K$</p>
<p>$$f(x)= \langle K(x, \cdot), f \rangle \in \mathcal{H}_K$$</p>
<p>$$K(x,y)= \langle K(x, \cdot), K(\cdot, y) \rangle _\mathcal{H}$$</p>
<p>(Mercer theorem) Note that for any $n \in \mathbb{N}, c_1, \dots, c_n \in \mathrm{R}, x_1,\dots,x_n \in \mathbb{R}$,</p>
<p>$$\begin{aligned} \sum_{i=1}^{n} \sum_{j=1}^{n} K(x_i, x_j) c_i c_j &amp;= \sum_{i=1}^{n} \sum_{j=1}^{n} \langle k_{x_i}, k_{x_j} \rangle _{\mathcal{H}} c_i c_j \\ &amp;= &lt;\sum_{i=1}^{n} c_i k_{x_i}, \sum_{j=i}^{n} c_j k_{x_j}&gt; \\ &amp;= ||\sum_{i=1}^{n} c_i k_{x_i}||_{\mathcal{H_k}}^{2}\geq 0 \end{aligned}$$</p>
<h3>Kernel</h3>
<p>Now let's focus on $\underset{f \in \mathcal{H}}{min} \sum_{i=1}^{n} D(y_i, f(x_i)) + \lambda ||f||_{\mathcal{H}_k}^2$</p>
<p>(Kernel trick) It can be shown that $f(x) = \sum_{j=1}^{n} \alpha_j K(x, x_j)$</p>
<p>$$\underset{\alpha_1,\dots,\alpha_n}{min} \sum_{i=1}^{n} D(y_i, \sum_{j=1}^{n} \alpha_j K(x, x_j)) + \lambda \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j K(x_i, x_j)$$</p>
<p>$$\Leftrightarrow \underset{\alpha}{min} \sum_{i=1}^{n} D(y_i, K \alpha) + \lambda \alpha^{\top} K \alpha \mathrm{,\space where\space} K=[K(x_i, x_j)]_{ij}$$</p>
<p>e.g. $D(y, K \alpha) = (y - K \alpha)^{\top} (y - K \alpha)$</p>
<p>$$\hat{\alpha} = (K^{\top}K + \lambda K)^{-1} K^{\top} y = (K + \lambda \mathbf{I}_n)^{-1} y$$</p>
<p>$$\hat{f} = K \hat{\alpha} = (\mathbf{I}_n + \lambda K^{-1})^{-1} y$$</p>
<p>For logistic regression, $D = \sum_{i=1}^{n} y f(x_i) - \log(1 + e^{f(x_i)})$</p>
<p>Iterative reweighted least squares (IRLS):</p>
<p>$$(K^{\top}w^{(t)}K + \lambda K) \alpha^{(t+1)} = K w^{(t)} \left[ K \alpha^{(t)} + w^{(t)^{-1}}(y - \mu^{(t)}) \right]$$</p>
<h2>NOTES 03/03</h2>
<h3>Kernel Smoothing</h3>
<p>Recall that we have advocated for a k-NN estimator in the beginning of the course. Assume for now that $\mathbf{X} \in \mathbb{R}$. If we want to estimate $f(\mathbf{x_0})$, then k-NN yields</p>
<p>$$\hat{f}(\mathbf{x}_0) = \frac{1}{k} \sum_{i \in N_k(\mathbf{x}_0)} y_i = \frac{\sum_{i=1}^{n}I[i \in N_k(\mathbf{x}_0)]y_i}{\sum_{i=1}^{n}I[i \in N_k(\mathbf{x}_0)]}$$</p>
<p>To avoid discontinuity we can define the estimator</p>
<p>$$\hat{f}(\mathbf{x}_0) = \frac{\sum_{i=1}^{n} K_{\lambda}(\mathbf{x}_0, \mathbf{x}_i) y_i}{\sum_{i=1}^{n} K_{\lambda}(\mathbf{x}_0, \mathbf{x}_i)}$$</p>
<p>where the &quot;Kernal&quot; $K_{\lambda}$ is defined as $K_{\lambda}(\mathbf{x}_0, \mathbf{x}) = D \left( \frac{|\mathbf{x} - \mathbf{x}_0|}{\lambda} \right)$, where $D$ is a density $\int D(t) dt = 1$</p>
<p>e.g. (1) $D = \phi$, the PDF of the standard normal
  (2) $D(t) = \frac{3}{4} (1-t^2)I[|t| \leq 1]$, &quot;Epanechnikov&quot;
  (3) $D(t) = (1-|t|^3)^3 I[|t| \leq 1]$, &quot;Tri-cubic&quot;</p>
<center><img src="./d.png" width='80%'></center>
<h3>Nadaraya–Watson Kernel Regression</h3>
<p>This estimator can be formalized by considering a model $f(\mathbf{x})=\alpha(\mathbf{x}_0)$</p>
<p>$$\hat{\alpha}(\mathbf{x}_0) = \underset{\alpha(\mathbf{x})}{\mathrm{argmin}} \sum_{i=1}^{n} K_{\lambda}(\mathbf{x}_0, \mathbf{x}_i)\left[ y_i - \alpha(\mathbf{x}_0) \right]^2$$</p>
<p>To further reduce bias, we can consider $f(\mathbf{x})=\alpha(\mathbf{x}_0) + \mathbf{x}^{\top} \beta(\mathbf{x}_0)$</p>
<p>$$(\hat{\alpha}(\mathbf{x}_0), \hat{\beta}(\mathbf{x}_0)) = \underset{\alpha(\mathbf{x}), \beta(\mathbf{x})}{\mathrm{argmin}} \sum_{i=1}^{n} K_{\lambda}(\mathbf{x}_0, \mathbf{x}_i)\left[ y_i - \alpha(\mathbf{x}) - \mathbf{x}_i^{\top} \beta(\mathbf{x})\right]^2$$</p>
<p>In general $f(\mathbf{x}_0) = b(\mathbf{x}_0)^{\top} \theta(\mathbf{x}_0)$, e.g.</p>
<p>$$b(\mathbf{x}_0)= \begin{bmatrix} 1 \\ \mathbf{x} \\ \mathbf{x}^2 \end{bmatrix} \mathrm{,\space} \theta(\mathbf{x}_0) = \begin{bmatrix} \theta_1(\mathbf{x}_0) \\ \theta_2(\mathbf{x}_0) \\ \theta_3(\mathbf{x}_0) \end{bmatrix}$$</p>
<p>Thus,</p>
<p>$$\begin{aligned} \hat{\theta}(\mathbf{x}_0) &amp;= \underset{\theta(\mathbf{x}_0)}{\mathrm{argmin}} \sum_{i=1}^{n} K_{\lambda}(\mathbf{x}_0, \mathbf{x}_i)\left[ y_i - b(\mathbf{x}_0)^{\top} \theta(\mathbf{x}_0) \right]^2 \\ &amp;= \underset{\theta(\mathbf{x}_0)}{\mathrm{argmin}} (\mathbf{y} - \mathbf{B}\theta (\mathbf{x}_0))^{\top} \underset{i=1,\dots,n}{\mathrm{Diag}} \left\{ K_{\lambda}(\mathbf{x}_0, \mathbf{x}_i) \right\} (\mathbf{y} - \mathbf{B}\theta (\mathbf{x}_0)) \end{aligned}$$</p>
<p>Let $\mathbf{W}_{\lambda}(\mathbf{x}_0) = \underset{i=1,\dots,n}{\mathrm{Diag}} \left\{ K_{\lambda}(\mathbf{x}_0, \mathbf{x}_i) \right\}$</p>
<p>We have</p>
<p>$$\hat{\theta}(\mathbf{x}_0) = (\mathbf{BW}_{\lambda}(\mathbf{x}_0)\mathbf{B})^{-1} \mathbf{B}^{\top} \mathbf{W}_{\lambda}(\mathbf{x}_0)\mathbf{y}$$</p>
<p>$$\hat{f}(\mathbf{x}_0) = b(\mathbf{x}_0)^{\top} \hat{\theta}(\mathbf{x}_0)=b(\mathbf{x}_0)^{\top} (\mathbf{BW}_{\lambda}(\mathbf{x}_0)\mathbf{B})^{-1} \mathbf{B}^{\top} \mathbf{W}_{\lambda}(\mathbf{x}_0)\mathbf{y}$$</p>
<p>Let $\mathbf{S}_{\lambda}(\mathbf{x}_0)=b(\mathbf{x}_0)^{\top} (\mathbf{BW}_{\lambda}(\mathbf{x}_0)\mathbf{B})^{-1} \mathbf{B}^{\top} \mathbf{W}_{\lambda}(\mathbf{x}_0)$</p>
<p>$$\hat{f} = [\mathbf{S}_{\lambda}(\mathbf{x}_i)^{\top}\mathbf{y}]_i = \mathbf{S}_{\lambda}\mathbf{y}$$</p>
<p>Thus, the degree of freedom $df_{\lambda} = tr(\mathbf{S}_{\lambda})$. When $df$ goes up, bias goes down and variance goes up.</p>
<p>We can also consider $K_{\lambda}(\mathbf{x}_0, \mathbf{x}) = D \left( \frac{|\mathbf{x} - \mathbf{x}_0|}{w(\mathbf{x}_0)} \right)$. Suppose now that $\mathbf{X} \in \mathbb{R}^p$, we can still use $f(\mathbf{x}_0) = b(\mathbf{x}_0)^{\top} \theta(\mathbf{x}_0)$.</p>
<p>For example, if $p=2$, $b(\mathbf{x}_0) = \begin{bmatrix} 1 &amp; \mathbf{x}_1 &amp; \mathbf{x}_2 &amp; \mathbf{x}_1 \mathbf{x}_2 &amp; \mathbf{x}_1^2 &amp; \mathbf{x}_2^2 \end{bmatrix}$</p>
<h3>Structured Kernel Regression</h3>
<p>As before $\hat{\theta}(\mathbf{x}_0) = \underset{\theta(\mathbf{x}_0)}{\mathrm{argmin}} \sum_{i=1}^{n} K_{\lambda}(\mathbf{x}_0, \mathbf{x}_i)\left[ y_i - b(\mathbf{x}_0)^{\top} \theta(\mathbf{x}_0) \right]^2$</p>
<p>We define $K_{\lambda}(\mathbf{x}_0, \mathbf{x}) = D \left( \frac{|\mathbf{x} - \mathbf{x}_0|}{w(\mathbf{x}_0)} \right)$ and a precision matrix $\mathbf{A}$</p>
<p>$$||\mathbf{x} - \mathbf{x}_0||_{\mathbf{A}}^2 = (\mathbf{x}- \mathbf{x}_0)^{\top}\mathbf{A}(\mathbf{x}-\mathbf{x}_0)$$</p>
<p>Another way to account for structure is to split the feature in to sets. $f(\mathbf{x}_0, \mathbf{z}_0) = b(\mathbf{x}_0)^{\top} \theta(\mathbf{z}_0)$. One for predicting, another for smoothing.</p>
<p>$$\hat{\theta}(\mathbf{z}_0) = \underset{\theta(\mathbf{z}_0)}{\mathrm{argmin}} \sum_{i=1}^{n} K_{\lambda}(\mathbf{z}_0, \mathbf{z}_i)\left[ y_i - b(\mathbf{x}_i)^{\top} \theta(\mathbf{z}_0) \right]^2$$</p>
<p>In general, if we have a likelihood model with $\log$ likelihood:</p>
<p>$$l(\mathbf{y}, \mathbf{X}) = \sum_{i=1}^{n}l(y_i, \mathbf{x}_i)$$</p>
<p>$$f(\mathbf{x}_0, \mathbf{z}_0) = g(b(\mathbf{x}_0)^{\top} \theta(\mathbf{z}_0))$$</p>
<p>Thus,</p>
<p>$$\hat{\theta}(\mathbf{z}_0) = \underset{\theta(\mathbf{z}_0)}{\mathrm{argmin}} \sum_{i=1}^{n} K_{\lambda}(\mathbf{z}_0, \mathbf{z}_i)\left[ y_i - g(b(\mathbf{x}_i)^{\top} \theta(\mathbf{z}_0)) \right]^2$$</p>
<p>e.g. logistic regression</p>
<p>$$f(\mathbf{x}_0, \mathbf{z}_0) = \mathbb{P}(y_0|\mathbf{x}_0, \mathbf{z}_0)=\mathrm{logit}^{-1} (b(\mathbf{x}_i)^{\top} \theta(\mathbf{z}_0))$$</p>
<p>$$l(y_i, g(b(\mathbf{x}_0)^{\top} \theta(\mathbf{z}_0))) = y_ib(\mathbf{x}_0)^{\top} \theta(\mathbf{z}_0) - \log (1+e^{b(\mathbf{x}_0)^{\top} \theta(\mathbf{z}_0)})$$</p>

                    <p style="border-top: 1px solid #dbdbdb; margin-top: 40px;"></p>
                    <div id="valine-thread"></div>
                    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
                    <script src='/js/Valine.min.js'></script>
                    <script>
                        new Valine({
                            el: '#valine-thread',
                            appId: 'DSTtKETb5UeyXIjoPTpRnC8Y-gzGzoHsz',
                            appKey: 'csHaHWqxD2Ujv84O7jaJWOSc',
                            verify: false,
                            avatar: 'robohash',
                            placeholder: '',
                            meta: ['nick', 'mail'],
                            requiredFields: ['nick'],
                            visitor: true,
                            lang: 'en'
                        })
                    </script>
                  
                    <footer>
                        <p style="padding: 5px 0 0 0; text-align: center; font-size: .9rem; border-top: 1px solid #dbdbdb;margin-top: 40px;">&#169; 2021 Jimmy Z. Hu<br>This website is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0.</a></p>
                    </footer>
                </div>
            </body>
        </html>
    