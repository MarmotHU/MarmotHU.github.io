
        <!DOCTYPE html>
        <html>
            <head>
                <meta http-equiv="Content-Type" content="text/html; charset=gb2312">
                <title>Clustering and PCA - Jimmy Z. Hu's Blog</title>
                <style rel="stylesheet" type="text/css">
                    body {font-family: Georgia; font-size: 17px;}
h2 {margin-top: 40px; border-bottom: 1px solid #999; text-transform: uppercase;}
p {text-align: left; line-height: 1.4em;}
pre {
    background: #f4f4f4; border: 1px solid #ddd; border-left: 3px solid #9f9f9f; 
    color: #666; page-break-inside: avoid; font-family: monospace; font-size: 15px; 
    line-height: 1.1; max-width: 100%; overflow: auto; padding: 0.5em 0.5em; 
    display: block; word-wrap: break-word;
}
li {line-height: 1.25em;}
a:link {text-decoration: none; color: blue;}
a:visited {text-decoration: none; color: blue;}
.container {width: 750px; margin-left: auto; margin-right: auto; padding: 50px 0 50px 0;}
.title {text-align: left;}
.katex {font-size: 1em !important;} 
table {width: 100%%;}
table, th, td {border: 1px solid black;}

                </style>
                <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
                <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
                <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
                <script>
                    document.addEventListener("DOMContentLoaded", function() {
                        renderMathInElement(document.body, {
                            delimiters: [
                              {left: "$$", right: "$$", display: true},
                              {left: "$", right: "$", display: false}
                            ] 
                        });
                    });
                </script>
            </head>
            <body>
                <div class="container">
                    <h1 class="title">Clustering and PCA</h1>
                    2020-04-21
                    <span style="float:right; _position:relative;">
                        <a href="/home">HOME</a>
                        &ensp;
                        <a href="/notes">NOTES</a> &ensp; <a href="../">RELATED</a>
                    </span><br/>
                    <h2>NOTES 04/21</h2>
<p>Recall that the main difference between supervised and unsupervised learning is that in the former we have a &quot;special&quot; feature as &quot;output&quot; $y$ and we don't highlight any feature in the latter.</p>
<ul>
<li>Supervised: $\mathbb{P}(\mathbf{x},y) = \mathbb{P}(y|\mathbf{x})\mathbb{p}(\mathbf{x})$</li>
<li>Unsupervised: $\mathbb{P}(\mathbf{x}, y)=\mathbb{P}(x_1,\dots,x_{p-1}, x_{p}=y)$</li>
</ul>
<p>The goal is to describe entire $\mathbb{P}(y|\mathbf{x})$ or $\mathbb{P}(\mathbf{x})$. For unsupervised learning, it is usual to describe $\mathbb{P}(\mathbf{x})$ using a simple model that captures the uncertainty in the data.</p>
<p><strong>Clustering</strong>: Considder a dataset $\left\{ \mathbf{x}_i \right\} _{i=1,\dots,N}$. The goal of clustering is to group observations in such a way that items within a cluster are more similar to each other than to items in other clusters. To this end, we need a metric to measure distance or similarity (the relationship between the two is monotonically decreasing).</p>
<p><u>Note</u> A former metric $d$ satisfies:</p>
<ul>
<li>$d(x,y)\geq 0$ with $d(x,y)=0$ if $x=y$;</li>
<li>$d(x,y)=d(y,x)$;</li>
<li>$d(x,y)\leq d(x,z)+d(z,y)$ (triangle inequality);</li>
</ul>
<p>Let us focus on distances and define $D(\mathbf{x}_i, \mathbf{x}_j)$ if there are $p$ features,</p>
<p>$$D(\mathbf{x}_i, \mathbf{x}_j)=\sum_{l=1}^{p}d_{l}(\mathbf{x}_{il}, \mathbf{x}_{jl})$$</p>
<p>Or more generally,</p>
<p>$$D(\mathbf{x}_i, \mathbf{x}_j)=\sum_{l=1}^{p}w_{l}d_{l}(\mathbf{x}_{il}, \mathbf{x}_{jl})$$</p>
<p>For instance, with $d_{l}(\mathbf{x}_{il}, \mathbf{x}_{jl})=h(|\mathbf{x}_{il} - \mathbf{x}_{jl}|) = (\mathbf{x}_{il} - \mathbf{x}_{jl})^2$, then $D(\mathbf{x}_i, \mathbf{x}_j)=||\mathbf{x}_i - \mathbf{x}_j||^2$.</p>
<p><u>Note</u> In general, similarity/distance is more inportant than methods! (A good distance/similarity is application specific and captures nuances in the data)</p>
<p>Let us then assume we have $K$ clusters. We asuume items to clusters using an encoder $C: \left\{ 1,\dots,N \right\} \rightarrow \left\{1,\dots,k\right\}$; $C(i) = k$ if $i$ belongs to cluster $k$. The goal is to define a loss and pick $C^{*}$ that minimizes the loss over all possible assignments.</p>
<p>Let's denote $d_{ij} = D(\mathbf{x}_i,\mathbf{x}_j)$ and define the overall distances $T = \frac{1}{2} \sum_{i=1}^{N}\sum_{j\not=i}d_{ij}$, which for any $C$ can be written</p>
<p>$$T = \frac{1}{2} \sum_{k=1}^{K}\sum_{i:C(i)=k} \left(\sum_{j:C(j)=k,j\not=i}d_{ij} + \sum_{l:C(l)=k}d_{il}\right)=W(C)+B(C)$$</p>
<p>where $W(C)= \frac{1}{2} \sum_{k=1}^{K}\sum_{i,j:i\not=j,C(i)=C(j)=k}d_{ij}$</p>
<p>In the spirit of clustering, we want</p>
<p>$$C^{*}=\underset{C}{\mathrm{argmin}}\space W(C) = \underset{C}{\mathrm{argmax}}\space B(C)$$</p>
<p>Let's focus on $W(C)$. Unfortunatelly, finding $C^{*}$ is very hard for large $N$ and $K$ since it's a combinational problem. There are</p>
<p>$$S(N,K) = \frac{1}{K!}\sum_{i=1}^{K}(-1)^{i}\biggl(\begin{smallmatrix}k\\i\end{smallmatrix}\biggr)(k-i)^N$$</p>
<p>Thus, we need (greedy) heuristics!</p>
<h3>K-Means Clustering</h3>
<p>This is a very popular method. Assume $L_2$ distance, $D(\mathbf{x}_i, \mathbf{x}_j)=||\mathbf{x}_i - \mathbf{x}_j||^2$, then,</p>
<p>$$\begin{aligned}W(C) &amp;= \frac{1}{2}\sum_{k=1}^{K}\sum_{i:C(i)=k}\sum_{i\not=j;C(j)=k}||\mathbf{x}_i-\mathbf{x}_j||^2 \\ &amp;= \sum_{k=1}^{K} N_k(C) \sum_{i:C(i)=k} ||\mathbf{x}_i-\overline{\mathbf{x}_k}||^2 \end{aligned}$$</p>
<p>where $N_k(C)=|\left\{i:C(i)=k\right\}|$ and $\overline{\mathbf{x}_k}=\frac{1}{N_k(C)}\sum_{i:C(i)=k}\mathbf{x}_i$. Thus we want</p>
<p>$$C^{*}=\underset{C}{\mathrm{argmin}}\sum_{k}N_k(C)\sum_{i:C(i)=k}||\mathbf{x}_i-\overline{\mathbf{x}_k}||^2$$</p>
<p>But we can augment the problem by realizing that $\overline{\mathbf{x}_k}=\underset{\mathbf{m}_k}{\mathrm{argmin}}\sum_{i:C(i)=k}||\mathbf{x}_i-\mathbf{m}_k||^2$</p>
<p>We can thus use cycle descent:</p>
<p><strong>Algorithm: K-Means</strong><br>
<strong>INITIALIZE</strong> C arbitrarily<br>
<strong>REPEAT</strong><br>
  $\mathbf{m}_k=\underset{\mathbf{m}}{\mathrm{argmin}}\sum_{i:C(i)=k}||\mathbf{x}_i-\mathbf{m}||$, $k=1,\dots,K$<br>
  $C(i)=\underset{k=1,\dots,K}{\mathrm{argmin}}\space||\mathbf{x}_i-\mathbf{m}_k||^2$, $i=1,\dots,N$<br>
<strong>UNTIL</strong> convergence of $W(C)$ (or $B(C)$)</p>
<p><u>Note</u></p>
<ul>
<li>As with many greedy procedure, start at multiple initializations and pick the best solution.</li>
<li>Similarly to EM/MM for mixture models, with $D$ defined from log-likelihood (e.g., Gaussian)</li>
<li>There are many possible ways to extend:
<ul>
<li>Instead of $D(\mathbf{x}_i,\mathbf{m}_k)=||\mathbf{x}_i-\mathbf{m}_k||^2$, we can use other metrics ($\mathbf{m}_k$ are Frechet means). This is more computationally expensive, but it might result in a better clustering.</li>
<li>E.G., K-Medoids: restrict $\mathbf{m}_k$ to be an observation; the criterion is then
$C^{*}=\underset{C}{\mathrm{argmin}} \underset{i_1,...,i_K}{\mathrm{min}}\ \sum_{k} \sum_{i:C(i)=k}D(\mathbf{x}_i,\mathbf{x}_{i_k})$ with $i_k^{*}=\underset{i:C(j)=k}{\mathrm{argmin}}\sum_{i:C(i)=k}D(\mathbf{x}_i,\mathbf{x}_j)$ being used in the first step in the iteration of K-Means ($\mathbf{k}_k=\mathbf{x}_{i_k}$).</li>
<li>Use multiple centroids per cluster (&quot;prototype&quot; methods)</li>
</ul>
</li>
</ul>
<p>Choosing K: The number of clusters $K$ here is a &quot;complexity&quot; parameter and should be regularized. There is no model-based criteria for doing so, but many criteria, e.g.,</p>
<ul>
<li>&quot;Gap Statistics&quot;, similar to scree plots;</li>
<li>Calinski-Harabasz (CA) Index:
$CH(C) = \frac{B(C)/(K-1)}{W(C)/(N-K)}$. Larger values are better! Inspired by ANOVA F-Statistic.</li>
</ul>
<h2>NOTES 04/23</h2>
<h3>Hierarchy Clustering</h3>
<p>We define a hierarchy where at each level we have a grouping of observations. The groups are nested, and can be represented in a tree (&quot;Dendrogram&quot;).</p>
<p>As we move up in the hierarchy, the higher the disparity between groups. There are two main ways to build the hhierarchy:</p>
<ul>
<li>Agglomeration (bottom-up)</li>
<li>Divisive (top-down)</li>
</ul>
<p>Let's focus on agglomerative clustering: we begin with each observation in its own group and for $N-1$ steps we merge the two least dissimllar goups. And define the threshold to control the model complexity. To this end, we need to define a group metric. Take two groups $G$ and $H$; the three most common metrics are:</p>
<ul>
<li>
<p>Single linkage: $d_{SL}(G,L)=\underset{i\in G,j \in H}{\mathrm{min}}d_{ij}$; draw back: &quot;chaining&quot;</p>
</li>
<li>
<p>Complete linkage: $d_{CL}(G,L)=\underset{i\in G,j \in H}{\mathrm{max}}d_{ij}$</p>
</li>
<li>
<p>Group average: $d_{GA}(G,L)=\frac{1}{N_G N_H}\sum_{i\in G,j \in H}d_{ij}$</p>
</li>
</ul>
<p>GA has a good compromise between SL &amp; CL, but it's less robust.</p>
<h3>PCA</h3>
<p>We have seen principal components before in the context of regression, as a way of identifying &quot;relevant&quot; features. It is more often used. However, in unsupervised learning as a way of &quot;compressing&quot; features, usually leading to clustering in a space of reduced dimemsion. Suppose then that we wish to use $q \leq p$ features to define the &quot;encoder&quot;:</p>
<p>$$f(\lambda)=\mu+\mathbf{v}_q \lambda$$</p>
<p>To &quot;compress&quot; $\mathbf{X}$, for instance, the $i$-th observation $\mathbf{x}_i$ is encoded by $f(\lambda)_i=\mu+\mathbf{v}_q \lambda_i$. Here $\mathbf{v}_q \in S_{p,q}$, i.e., the columns of $\mathbf{v}_q$ are orthogonal. Our goal is to learn</p>
<p>$$(\hat{\mu},\hat{\mathbf{v}_q}, \hat{\lambda}) = \underset{\mu, \mathbf{v}_q, \lambda}{\mathrm{argmin}} \sum_{i=1}^{N} ||\mathbf{x}_i - (\mu + \mathbf{v}_q \lambda_i)||^2$$</p>
<p>It can be shown that $\mu=\overline{\mathbf{x}}$ and so</p>
<p>$$\underset{\mathbf{v}_q, \lambda}{\mathrm{min}} \sum_{i=1}^{N}||\mathbf{x}_i-\overline{\mathbf{x}}-\mathbf{v}_q\lambda_i||^2 = \underset{\mathbf{v}_q, \Lambda}{\mathrm{min}} ||\tilde{\mathbf{X}}-\Lambda \mathbf{v}_q||^2_F$$</p>
<p>where $\tilde{\mathbf{X}}=\mathbf{X}-\mathbf{1}\overline{\mathbf{x}}^{\top}$.</p>
<p>By the Eckart-Young theorem, if $\tilde{\mathbf{X}}$ has $\tilde{\mathbf{X}}=\mathbf{U}_q\mathbf{D}_q\mathbf{V}_q^{\top}$, then $\Lambda\mathbf{V}_q=\mathbf{U}_q\mathbf{D}_q\mathbf{V}_q^{\top} \Rightarrow \Lambda=\mathbf{U}_q\mathbf{D}_q=\tilde{\mathbf{X}}\mathbf{V}_q$. Recall that $\mathbf{V} \in S_{N,p}$, $\mathbf{D}=\mathrm{Diag}\left\{d_i\right\}$, $d_i \geq d_2 \geq \dots \geq d_p$, $\mathbf{V} \in O(p)$, note that $\Lambda = \begin{bmatrix} \lambda_1^{\top} \\ \vdots \\ \lambda_N^{\top} \end{bmatrix} = \begin{bmatrix} \mathbf{z}_1 &amp; \cdots &amp; \mathbf{z}_q \end{bmatrix}$, so $f(\lambda)_i = \overline{\mathbf{x}} + \sum_{j=1}^{q} \lambda_{ij}v_j$</p>
<p>One way to abstract this representation is to define $\lambda_f(\mathbf{x})=\underset{\lambda}{\mathrm{argmin}} ||\mathbf{x}-f(\lambda)||^2$, and thing of $f(\lambda)$ as $f(\lambda) = \mathbb{E}[\mathbf{x}|\lambda_f(\mathbf{x})=\lambda]$</p>
<p>Suppose, for now, that $q=1$ so $f(\lambda)=f(\lambda)_1,\dots, f(\lambda)_J$. To estimate $f$, we just need to iterate between two steps:</p>
<ul>
<li>
<p>$\hat{f}_j(\lambda) = \mathbb{E}[\mathbf{x}_j|\lambda_f(\mathbf{x})=\lambda], j=1,\dots,p$</p>
</li>
<li>
<p>$\hat{\lambda}_f(\mathbf{x})=\underset{\lambda}{\mathrm{argmin}}||\mathbf{x}-\hat{f}(\mathbf{x})||^2$</p>
</li>
</ul>
<p>In general,</p>
<p>$$f(\lambda_1, \dots, \lambda_q) = (f_1(\lambda_1, \dots, \lambda_q), \dots, f_p(\lambda_1, \dots, \lambda_q))$$</p>
<h3>Kernel PCA</h3>
<p>Recall that the PCs are $\mathbf{Z}=\mathbf{U}\mathbf{D}=\mathbf{X}\mathbf{V}$ ($\mathbf{X}$ has been centered). We can obtain $\mathbf{Z}$ from the eigendecomposition $\mathbf{XX}^{\top} = \mathbf{U}\mathbf{D}^2\mathbf{U}^{\top}$.</p>
<p>Since we only need $\mathbf{XX}^{\top} = [\lang\mathbf{x}_i, \mathbf{x}_j \rang]_{i,j}$, we can exploit &quot;kernel trick&quot; by representing $\mathbf{K} = [\lang\phi(\mathbf{x}_i), \phi(\mathbf{x}_j) \rang]=[K(\mathbf{x}_i, \mathbf{x}_j)]_{i,j}$. For the implicit non-linear trainsformation $\phi$ of the feature in this case,</p>
<p>$$\mathbf{Z}=\mathbf{U}\mathbf{D}=\mathbf{U}\mathbf{D}^2\mathbf{U}^{\top}\mathbf{U}\mathbf{D}^{-1}=\mathbf{K}\mathbf{U}\mathbf{D}^{-1}$$</p>
<p>A useful interpretation of PCA is following: it can shown that, in regular PCA, the orthogonal right vectors $\mathbf{v}_1, \dots, \mathbf{v}_p \in \mathbb{R}^p$ that spacify:</p>
<p>$$\mathbf{v}_1=\underset{\mathbf{v}:||\mathbf{v}||=1}{\mathrm{argmin}} \hat{\mathrm{Var}}_{\tau}(f(\mathbf{x}))$$</p>
<p>$$\mathbf{v}_j=\underset{\substack{\mathbf{v}:||\mathbf{v}||=1,\\ \lang \mathbf{v},\mathbf{v}_k \rang=0, k=1,\dots,j-1}}{\mathrm{argmin}} \hat{\mathrm{Var}}_{\tau}(f(\mathbf{x}))$$</p>
<p>In kernel PCA, we have PC functions $f_1,\dots,f_p \in \mathscr{H}_k$ (RKHS):</p>
<p>$$f_1=\underset{f:||f||_{\mathscr{H}_k}=1}{\mathrm{argmin}} \hat{\mathrm{Var}}_{\tau}(f(\mathbf{x}))$$</p>
<p>$$f_j=\underset{\substack{f:||f||_{\mathscr{H}_k}=1, \\ \lang f, f_k \rang_{\mathscr{H}_k}=0, k=1,\dots,j-1}}{\mathrm{argmin}} \hat{\mathrm{Var}}_{\tau}(f(\mathbf{x}))$$</p>

                    <p style="border-top: 1px solid #dbdbdb; margin-top: 40px;"></p>
                    <div id="valine-thread"></div>
                    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
                    <script src='/js/Valine.min.js'></script>
                    <script>
                        new Valine({
                            el: '#valine-thread',
                            appId: 'DSTtKETb5UeyXIjoPTpRnC8Y-gzGzoHsz',
                            appKey: 'csHaHWqxD2Ujv84O7jaJWOSc',
                            verify: false,
                            avatar: 'robohash',
                            placeholder: '',
                            meta: ['nick', 'mail'],
                            requiredFields: ['nick'],
                            visitor: true,
                            lang: 'en'
                        })
                    </script>
                  
                    <footer>
                        <p style="padding: 5px 0 0 0; text-align: center; font-size: .9rem; border-top: 1px solid #dbdbdb;margin-top: 40px;">&#169; 2021 Jimmy Z. Hu<br>This website is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0.</a></p>
                    </footer>
                </div>
            </body>
        </html>
    