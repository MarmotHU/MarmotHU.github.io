
        <!DOCTYPE html>
        <html>
            <head>
                <meta http-equiv="Content-Type" content="text/html; charset=gb2312">
                <title>Neural Network - Jimmy Z. Hu's Blog</title>
                <style rel="stylesheet" type="text/css">
                    body {font-family: Georgia; font-size: 17px;}
h2 {margin-top: 40px; border-bottom: 1px solid #999; text-transform: uppercase;}
p {text-align: left; line-height: 1.4em;}
pre {
    background: #f4f4f4; border: 1px solid #ddd; border-left: 3px solid #9f9f9f; 
    color: #666; page-break-inside: avoid; font-family: monospace; font-size: 15px; 
    line-height: 1.1; max-width: 100%; overflow: auto; padding: 0.5em 0.5em; 
    display: block; word-wrap: break-word;
}
li {line-height: 1.25em;}
a:link {text-decoration: none; color: blue;}
a:visited {text-decoration: none; color: blue;}
.container {width: 750px; margin-left: auto; margin-right: auto; padding: 50px 0 50px 0;}
.title {text-align: left;}
.katex {font-size: 1em !important;} 
table {width: 100%%;}
table, th, td {border: 1px solid black;}

                </style>
                <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
                <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
                <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
                <script>
                    document.addEventListener("DOMContentLoaded", function() {
                        renderMathInElement(document.body, {
                            delimiters: [
                              {left: "$$", right: "$$", display: true},
                              {left: "$", right: "$", display: false}
                            ] 
                        });
                    });
                </script>
            </head>
            <body>
                <div class="container">
                    <h1 class="title">Neural Network</h1>
                    2020-04-07
                    <span style="float:right; _position:relative;">
                        <a href="/home">HOME</a>
                        &ensp;
                        <a href="/notes">NOTES</a> &ensp; <a href="../">RELATED</a>
                    </span><br/>
                    <h2>NOTES 04/07</h2>
<p>A neural network is a multi-stage regression model with &quot;hidden&quot; layers and k-dimensional output. Suppose that we have <strong>a single hidden layer</strong> with $M$ features and $P$ observed features.</p>
<p>The model is $f_k(\mathbf{x})=g_k(\beta_{0,k} + \mathbf{z}^{\top}\beta_k)^{\top}$, where $\mathbf{z}_m = \sigma(\alpha_{0, m}+\mathbf{x}^{\top}\alpha_m)$.</p>
<p>Usually, activation function $\sigma(v) = \frac{1}{1+e^{-v}}=\mathrm{logit}^{-1}(v)$.</p>
<p>To fit a neural network we need to estimate the weights $\theta = (\left\{\alpha_{0,m},\alpha_m \right\},\left\{\beta_{0,m},\beta_m \right\})$, As usuall, we assume a data fitting loss:</p>
<p>$$\mathrm{R}(\theta) = \sum_{i=1}^{N} \sum_{k=1}^{K} \mathrm{L}_{ik} (y_{ik}, f_k(\mathbf{x}_i))$$</p>
<p>E.G,</p>
<ul>
<li>
<p>Squared error: $\mathrm{R}(\theta) = \sum_{i, k} \left(y_{ik}, f_k(\mathbf{x}_i)\right)^2$</p>
</li>
<li>
<p>Cross-entropy or deviance: $\mathrm{R}(\theta) = - \sum_{i, k} y_{ik} \log f_k(\mathbf{x}_i)$</p>
</li>
</ul>
<p>To obtain $\hat{\theta} = \underset{\theta}{\mathrm{argmin}} \space \mathrm{R}(\theta)$ we use gradient descent. With $\mathrm{R}(\theta)=\sum \mathrm{R}_i(\theta)$, let us, for simplicity, set $\mathbf{x} = \begin{bmatrix} 1 &amp; x\end{bmatrix}$ and $\mathbf{z} = \begin{bmatrix} 1 &amp; z \end{bmatrix}$ so that $z_{mi}=\sigma(\mathbf{x}_i^{\top}\alpha_m)$, $\mathbf{z}_i = (z_{1i},\dots,z_{Mi})$, and $f_k(\mathbf{x}_i)=g_k(\mathbf{z}_i^{\top}\beta_k)$</p>
<p>$$\begin{aligned} \frac{\partial \mathrm{R}_i}{\partial \beta_{km}} &amp;= \frac{\partial \mathrm{R}_i}{\partial f_{ki}} \frac{\partial f_{ki}}{\partial \beta_{km}} \\ &amp;= \frac{\partial \mathrm{L}_{ki}}{\partial f_{ki}} \cdot g^{\prime}_k(\mathbf{z}_i^{\top}\beta_k)\cdot z_{mi} = \delta_{ki} z_{mi} \\ \frac{\partial \mathrm{R}_i}{\partial \alpha_{ml}} &amp;= \sum_{k=1}^{K} \frac{\partial \mathrm{R}_i}{\partial f_{ki}} \frac{\partial f_{ki}}{\partial z_{mi}} \frac{\partial z_{mi}}{\partial \alpha_{ml}} \\ &amp;= \sum_{k=1}^{K} \frac{\partial \mathrm{L}_{ki}}{\partial f_{ki}} g^{\prime}_k (\mathbf{z}_i^{\top} \beta_k) \beta_{km} \cdot \sigma^{\prime} (\mathbf{x}_i^{\top} \alpha_m) \cdot x_{il} \\ &amp;= \sigma^{\prime} (\mathbf{x}_i^{\top} \alpha_m) \sum_{k=1}^{K} \delta_{ki} \beta_{km} \cdot x_{il} \\ s_{mi} &amp;= \sigma^{\prime}(\mathbf{x}_i^{\top}\alpha_m) \sum_{k=1}^K \delta_{ki} \beta_{km}\end{aligned}$$</p>
<p>which is called &quot;back-propagation equations&quot;. For the updates:</p>
<p>$$\begin{aligned} \beta_{km}^{(t)} &amp;= \beta_{km}^{(t-1)} - \gamma_t \frac{\partial \mathrm{R}(\theta^{(t-1)})}{\partial \beta_{km}}  \\ &amp;= \beta_{km}^{(t-1)} - \gamma_t \cdot \sum_{i=1}^{N} \frac{\partial \mathrm{R}_i (\theta^{(t-1)})}{\partial \beta_{km}}  \\ \alpha_{ml}^{(t)} &amp;= \alpha_{ml}^{(t-1)} - \gamma_t \cdot \sum_{i=1}^{N} \frac{\partial \mathrm{R}_i (\theta^{(t-1)})}{\partial \alpha_{ml}}  \end{aligned}$$</p>
<p>where $\gamma_t$ is called &quot;step size&quot; or &quot;learning rate&quot;.</p>
<p>$\gamma_t$ can be constant and be &quot;calibrated&quot; using line search; it can also be reduced to ensure convergence by setting a schedule with $\gamma_t \rightarrow 0$ with $\sum_{t} \gamma_t = \infty$ and $\sum_{t} \gamma_t^{2} &lt; \infty$ (e.g., $\gamma_t = \frac{1}{t}$)</p>
<p>Thus, to fit the model we can iterate as follows, until convergence:</p>
<ol>
<li>
<p>&quot;Forward pass&quot;: $\mathbf{X} \rightarrow \mathbf{Z} \rightarrow \mathbf{Y}$</p>
<ul>
<li>
<p>Given $\theta^{(t-1)} = \left(\left\{\alpha_m^{(t-1)} \right\}_{m=1,\dots,M},\left\{\beta_k^{(t-1)} \right\}_{k=1,\dots,K}\right)$</p>
</li>
<li>
<p>Set $z_{mi}^{(t-1)} = \sigma \left(\mathbf{x}_c^{\top}\alpha_m^{(t-1)} \right)_{m=1,\dots,M}$, $f_{ki}^{(t-1)} = g_k \left(z_{mi}^{\top}\beta_k^{(t-1)}\right)_{k=1,\dots,K}$</p>
</li>
</ul>
</li>
<li>
<p>&quot;Backward Pass&quot;: $\mathbf{X} \leftarrow \mathbf{Z} \leftarrow \mathbf{Y}$</p>
<ul>
<li>
<p>$g_{ki}^{(t-1)}=\frac{\partial \mathrm{L}_{ki}(y_i, f_{ki}^{(t-1)})}{\partial f_{ki}} \cdot g_k^{\prime}(z_{m-1}^{(t-1)^{\top}}\beta_k^{(t-1)})$</p>
</li>
<li>
<p>$s_{mi}^{(t-1)} = \sigma^{\prime}(\mathbf{x}_i^{\top}\alpha_m^{(t-1)}) \sum_{k=1}^K \delta_{ki} \beta_{km}^{(t-1)}$</p>
</li>
</ul>
</li>
<li>
<p>Update</p>
<ul>
<li>
<p>$\beta_{km}^{(t)} = \beta_{km}^{(t-1)} - \gamma_t \cdot \sum_{i=1}^{N} \delta_{ki}^{(t-1)}z_{mi}^{(t-1)}$</p>
</li>
<li>
<p>$\alpha_{ml}^{(t)} = \alpha_{ml}^{(t-1)} - \gamma_t \cdot \sum_{i=1}^{N} s_{mi}^{(t-1)} x_{il}$</p>
</li>
</ul>
</li>
</ol>
<p>The B.P algorithm as stated here only optimizes data fitting loss; ideally, however, we should also account for model complexity and set as a criterion:</p>
<p>$$\hat{\theta} = \underset{\theta}{\mathrm{R}(\theta)} + \lambda J(\theta)$$</p>
<p>E.G., $J(\theta) = \sum_{k=1}^{K} ||\beta_k||_{2}^{2} + \sum_{m=1}^{M} ||\alpha_m||_{2}^{2}$, &quot;Ridge&quot;.</p>
<p>Notes:</p>
<ul>
<li>
<p>We can add more hidden layers and still use B.P. (&quot;Deep Learning&quot;).</p>
</li>
<li>
<p>We don't have to have a fully connected network at each layer and, moreover, we can share weights (&quot;Convolutional Neural Network&quot;).</p>
</li>
<li>
<p>Modern approaches for &quot;automatic&quot;: B.P. &quot;Differential programming&quot;.</p>
</li>
<li>
<p>Check: TensorFlow | Keras</p>
</li>
<li>
<p>The parameter space is full of local minima, so it is usual to initialize the weights randomly and run multiple times to select the best solution. Thus, it is good to rely on well implemented and documented libraries.</p>
</li>
<li>
<p>Adding more layers is similar to extending basis in forward stagewise additive modeling; however, while in FSAM the &quot;weights&quot; are fit and then fixed at each iteration, due to back propagation, all weights are updates in neural network.</p>
</li>
</ul>

                    <p style="border-top: 1px solid #dbdbdb; margin-top: 40px;"></p>
                    <div id="valine-thread"></div>
                    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
                    <script src='/js/Valine.min.js'></script>
                    <script>
                        new Valine({
                            el: '#valine-thread',
                            appId: 'DSTtKETb5UeyXIjoPTpRnC8Y-gzGzoHsz',
                            appKey: 'csHaHWqxD2Ujv84O7jaJWOSc',
                            verify: false,
                            avatar: 'robohash',
                            placeholder: '',
                            meta: ['nick', 'mail'],
                            requiredFields: ['nick'],
                            visitor: true,
                            lang: 'en'
                        })
                    </script>
                  
                    <footer>
                        <p style="padding: 5px 0 0 0; text-align: center; font-size: .9rem; border-top: 1px solid #dbdbdb;margin-top: 40px;">&#169; 2021 Jimmy Z. Hu<br>This website is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0.</a></p>
                    </footer>
                </div>
            </body>
        </html>
    