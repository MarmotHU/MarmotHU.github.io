
        <!DOCTYPE html>
        <html>
            <head>
                <meta http-equiv="Content-Type" content="text/html; charset=gb2312">
                <title>Extending PCA and Spectral Clustering - Jimmy Z. Hu's Blog</title>
                <style rel="stylesheet" type="text/css">
                    body {font-family: Georgia; font-size: 17px;}
h2 {margin-top: 40px; border-bottom: 1px solid #999; text-transform: uppercase;}
p {text-align: left; line-height: 1.4em;}
pre {
    background: #f4f4f4; border: 1px solid #ddd; border-left: 3px solid #9f9f9f; 
    color: #666; page-break-inside: avoid; font-family: monospace; font-size: 15px; 
    line-height: 1.1; max-width: 100%; overflow: auto; padding: 0.5em 0.5em; 
    display: block; word-wrap: break-word;
}
li {line-height: 1.25em;}
a:link {text-decoration: none; color: blue;}
a:visited {text-decoration: none; color: blue;}
.container {width: 750px; margin-left: auto; margin-right: auto; padding: 50px 0 50px 0;}
.title {text-align: left;}
.katex {font-size: 1em !important;} 
table {width: 100%%;}
table, th, td {border: 1px solid black;}

                </style>
                <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
                <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
                <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
                <script>
                    document.addEventListener("DOMContentLoaded", function() {
                        renderMathInElement(document.body, {
                            delimiters: [
                              {left: "$$", right: "$$", display: true},
                              {left: "$", right: "$", display: false}
                            ] 
                        });
                    });
                </script>
            </head>
            <body>
                <div class="container">
                    <h1 class="title">Extending PCA and Spectral Clustering</h1>
                    2020-05-05
                    <span style="float:right; _position:relative;">
                        <a href="/home">HOME</a>
                        &ensp;
                        <a href="/notes">NOTES</a> &ensp; <a href="../">RELATED</a>
                    </span><br/>
                    <h2>NOTES 04/28</h2>
<h3>Sparse Principal Components</h3>
<p>Recall that if $\mathbf{X}_{N \times p}$ has centered columns, then $q \leq p$ principal components of $\mathbf{X}$ are defined by</p>
<p>$$\begin{aligned} &amp; \underset{\substack{\pmb{\Lambda}, \mathbf{V} \in S_{p,q} \\ (\mathbf{V}^{\top}\mathbf{V}=\mathbf{I}_q)}}{\min} \sum_{i=1}^{N}||\mathbf{x}_i - \mathbf{V}\lambda_i||^2 \\ =&amp; \underset{\mathbf{V} \in S_{p,q}}{\min} \sum_{i=1}^{N}||\mathbf{x}_i - \mathbf{V}\mathbf{V}^{\top}\mathbf{x}_i||^2 \end{aligned}$$</p>
<p>and $\pmb{\Lambda} = \begin{bmatrix} \lambda_1^{\top} \\ \vdots \\ \lambda_N^{\top} \end{bmatrix} = \mathbf{X}\mathbf{V}$ are principal components. We can use $\mathbf{V}$ to interpret PCA since they provide directions of largest variance. To this end, it is useful if we make them sparse by a penalty:</p>
<p>$$\underset{\mathbf{V} \in S_{p,q}}{\min} \sum_{i=1}^{N}||\mathbf{x}_i - \mathbf{V}\mathbf{V}^{\top}\mathbf{x}_i||^2+\lambda \sum_{j=1}^{p} \sum_{k=1}^{q} |v_{jk}|$$</p>
<p>(LASSO penalty, &quot;SCoTLASS&quot;)</p>
<p>The $L_1$ penalty encourages sparsity by selecting and shrinking to zero. Unfortunately, this is not convex, and so hard to fit in practice. Another sparse PC approach is due to Zou et al. (2006):</p>
<p>$$ \underset{\pmb{\Theta},\mathbf{V}}{\min} \sum_{i=1}^{N}||\mathbf{x}_i - \mathbf{V}\lambda_i||^2+w\sum_{k=1}^{q}||\mathbf{v}_k||_2^2 + \sum_{k=1}^{q}\lambda_k ||\mathbf{v}_k||_1$$</p>
<p>where this criterion is not jointly convex on $\pmb{\Theta}$ and $\mathbf{V}$, it is convex on each parameter having the other parameter fixed. Thus we can proceed with cyclic descent: with $\pmb{\Theta}$ fixed, we fit $\mathbf{v}_k, k=1,\dots,q$, using elestic net; with $\mathbf{V}$ fixed we just need a simple SVD to update $\pmb{\Theta}$.</p>
<h3>Multidimensional Scaling (MDS)</h3>
<p>Recall that we can use principal curves/surfaces to &quot;embed&quot; the data in a manifold of lower dimension. In MDS, the goal is similar, but the embedding is clear: given $\mathbf{x}_1,\dots,\mathbf{x}_N \in \mathbb{R}^p$ and $d_{ij} = D(\mathbf{x}_i, \mathbf{x}_j)$, e.g., $d_{ij} = ||\mathbf{x}_i - \mathbf{x}_j||_1$. We seek $\mathbf{z}_1,\dots,\mathbf{z}_N \in \mathbb{R}^q$ that minimize the &quot;stress&quot; function</p>
<p>$$S_M(\mathbf{Z})=\sum_{i\not = j} \left(d_{ij}-||\mathbf{z}_i - \mathbf{z}_j|| \right)^2$$</p>
<p>(&quot;Kruskal-Shepard Scaling&quot;)</p>
<p>The new positions $\mathbf{Z}$ are a representation that preserves the distances as well as possible. Note that the distances $d_{ij}$ can be implicit, that is, we don't need $\mathbf{x}_1,\dots,\mathbf{x}_N$.</p>
<p>We can also start with similarities. Instead, for instance, $s_{ij} = \lang \mathbf{x}_i - \overline{\mathbf{x}}, \mathbf{x}_j - \overline{\mathbf{x}} \rang$, we then want $\mathbf{Z} = (\mathbf{z}_i),\dots,\mathbf{z}_N$ so that</p>
<p>$$S_C(\mathbf{Z})=\sum_{i,j} \left(s_{ij}-\lang \mathbf{x}_i - \overline{\mathbf{x}}, \mathbf{x}_j - \overline{\mathbf{x}} \rang \right)^2$$</p>
<p>(&quot;Classical Scaling&quot;) is minimized. But then</p>
<p>$$S_C(\mathbf{Z})=\sum_{i,j} (\tilde{\mathbf{x}}_i^{\top}\tilde{\mathbf{x}}_j - \tilde{\mathbf{z}}_i^{\top}\tilde{\mathbf{z}}_j)^2=||\tilde{\mathbf{X}}^{\top}\tilde{\mathbf{X}} - \tilde{\mathbf{Z}}^{\top}\tilde{\mathbf{Z}}||_F^2$$</p>
<p>And so, by the Eckart-Young Theorem if $\tilde{\mathbf{X}}=\mathbf{UDV}^{\top}$ then $\tilde{\mathbf{X}}\tilde{\mathbf{X}}^{\top} = \mathbf{UD}^2\mathbf{V}$ and $\tilde{\mathbf{Z}}\tilde{\mathbf{Z}}^{\top} = \mathbf{U}_q\mathbf{D}^{2}_q\mathbf{V}_q$, then is $\tilde{\mathbf{Z}}=\mathbf{U}_q\mathbf{D}_q$, the pricipal components.</p>
<p>A generalization is Shepard-Kruskal nonmetric scaling with stress:</p>
<p>$$S_{NM}(\mathbf{Z})=\frac{\sum_{i\not = j} \left(\theta(d_{ij})-||\mathbf{z}_i - \mathbf{z}_j|| \right)^2}{\sum_{i\not = j}||\mathbf{z}_i - \mathbf{z}_j||^2}$$</p>
<p>where $\theta$ is an arbitrary increasing function. The function $theta$ and positions $\mathbf{Z}$ are minimized via cyclic descent: with $\theta$ fixed, $\mathbf{Z}$ is updated with gradient descent; with $\mathbf{Z}$ fixed, $\theta$ is fit with isotonic regression.</p>
<h3>Non-negative Matrix Decomposition</h3>
<p>The PC embedding can be relaxed to</p>
<p>$$\underset{\mathbf{V},\pmb{\Lambda}}{\min} \sum_{i=1}^N \sum_{j=1}^P (\mathbf{x}_{ij} - (\pmb{\Lambda} \mathbf{V}^{\top})_{ij})^2$$</p>
<p>with $(\pmb{\Lambda} \mathbf{V}^{\top})_{ij}=\sum_{k=1}^q \lambda_{ik}\mathbf{v}_{jk}$ and $\mathbf{X} \approx \pmb{\Lambda} \mathbf{V}^{\top}$</p>
<p>We can extend this definition to accommodate more general losses:</p>
<p>$$\underset{\mathbf{V},\pmb{\Lambda}}{\min} \sum_{i=1}^N \sum_{j=1}^P L \left(x_{ij}, f \left( (\pmb{\Lambda} \mathbf{V}^{\top})_{ij} \right) \right)$$</p>
<p>For instance, taking $L$ to be the poisson deviance (negative log-likelihood) loss ($x_{ij} \overset{i.i.d}{\sim} \mathrm{Po}(\mu_{ij})$),</p>
<p>$$\underset{\mathbf{V},\pmb{\Lambda}}{\min} \sum_{i=1}^N \sum_{j=1}^P x_{ij} \log (\pmb{\Lambda} \mathbf{V}^{\top})_{ij} - (\pmb{\Lambda} \mathbf{V}^{\top})_{ij}$$</p>
<p>where $(\pmb{\Lambda} \mathbf{V}^{\top})_{ij} = \mu_{ij}$</p>
<p>Then $\pmb{\Lambda}$ and $\mathbf{V}^T$ form a non-negative matrix factorization since $(\pmb{\Lambda} \mathbf{V}^{\top})_{ij} \geq 0$. While we could, in principle, use the same optimization machinery from GLMs here, a simpler approach is based, again on cyclic descent:</p>
<p>$$\lambda_{ik} \leftarrow \lambda_{ik} \frac{\sum_{j=1}^{P} v_{jk} x_{ij} / (\pmb{\Lambda} \mathbf{V}^{\top})_{ij}}{\sum_{j=1}^{P} v_{jk}}$$</p>
<p>$$v_{ik} \leftarrow v_{ik} \frac{\sum_{i=1}^{N} \lambda_{ik} x_{ij} / (\pmb{\Lambda} \mathbf{V}^{\top})_{ij}}{\sum_{i=1}^{N} v_{ik}}$$</p>
<p><u>NOTE</u> That $\mathbf{V}$ and $\pmb{\Lambda}$ are not identifiable since, for $\mathbf{R} \in O(q)$,</p>
<p>$$\pmb{\Lambda} \mathbf{V}^{\top} = \pmb{\Lambda} \mathbf{RR}^{\top}\mathbf{V}^{\top} = (\pmb{\Lambda} \mathbf{R})(\mathbf{VR})^{\top} = \pmb{\Lambda}^{*}\mathbf{V^{*}}^{\top}$$</p>
<h2>NOTES 04/30</h2>
<h3>Spectral Clustering</h3>
<p>Traditional clustering methods assume an elliptical metric when grouping, but don't work well if the clusters are not convex.</p>
<p>Spectral clustering is a generalization that, similar to MDS, assumes implicit similarities $s_{ij}$ between observations $i$ and $j$.</p>
<center><img src="./2.png" width="60%"></center>
<p>If these relations are represented in an undirected graph $G=(V,E)$ where $V= \left\{ 1,\dots,N \right\}$ and $s_{ij}=0$ implies $(i,j)\not \in E$ and $e=(i,j)$ has weight given by $s_{ij} &gt; 0$ if it exceeds threshold, then the goal of spectral clustering is to partition $G$ such that within-group edges have high weight (similarity) and between-group edges have low weights.</p>
<p>The most popular way to define a similarity graph is to use Mutual K-Nearest Neighbor Graph: if $N_k(v)$ is the k-neighborhood of $v$, the set of observations with the $k$ largest similarities with $v$, then</p>
<p>$$w_{ij} = s_{ij} I (i \in N_k(j) \mathrm{\space and \space} j \in N_k(i))$$</p>
<p>The matrix $\mathbf{W} = [w_{ij}]$ is the (weighted) adjacency matrix of $G$. The degree of $v$, $d_v=\sum_{u\not=v}w_{uv}$, and we define $\mathbf{D}=\underset{v \in V}{\mathrm{Dias}} \left\{ d_v \right\}$. The graph Laplacian $\mathbf{L}=\mathbf{D}-\mathbf{W}$ captures the &quot;connectivity&quot; of $G$. Since $\mathbf{L}\mathbf{1}_N=0$, $\mathbf{1}_N$ is a trivial eigenvector of eigenvalue zero, but, in general, the multiplicity of the zero eigenvalue is the number of (connected) components of $G$. Thus, checking the spectral decomposition of $L$ and the magnitude of the eigenvalues in increasing order can inform the number of clusters while the eigenvectors indicate group membership.</p>
<center><img src="./1.png" width="60%"></center>

                    <p style="border-top: 1px solid #dbdbdb; margin-top: 40px;"></p>
                    <div id="valine-thread"></div>
                    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
                    <script src='/js/Valine.min.js'></script>
                    <script>
                        new Valine({
                            el: '#valine-thread',
                            appId: 'DSTtKETb5UeyXIjoPTpRnC8Y-gzGzoHsz',
                            appKey: 'csHaHWqxD2Ujv84O7jaJWOSc',
                            verify: false,
                            avatar: 'robohash',
                            placeholder: '',
                            meta: ['nick', 'mail'],
                            requiredFields: ['nick'],
                            visitor: true,
                            lang: 'en'
                        })
                    </script>
                  
                    <footer>
                        <p style="padding: 5px 0 0 0; text-align: center; font-size: .9rem; border-top: 1px solid #dbdbdb;margin-top: 40px;">&#169; 2021 Jimmy Z. Hu<br>This website is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0.</a></p>
                    </footer>
                </div>
            </body>
        </html>
    