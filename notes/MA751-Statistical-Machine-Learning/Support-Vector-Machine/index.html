
        <!DOCTYPE html>
        <html>
            <head>
                <meta http-equiv="Content-Type" content="text/html; charset=gb2312">
                <title>Support Vector Machine - Jimmy Z. Hu's Blog</title>
                <style rel="stylesheet" type="text/css">
                    body {font-family: Georgia; font-size: 17px;}
h2 {margin-top: 40px; border-bottom: 1px solid #999; text-transform: uppercase;}
p {text-align: left; line-height: 1.4em;}
pre {
    background: #f4f4f4; border: 1px solid #ddd; border-left: 3px solid #9f9f9f; 
    color: #666; page-break-inside: avoid; font-family: monospace; font-size: 15px; 
    line-height: 1.1; max-width: 100%; overflow: auto; padding: 0.5em 0.5em; 
    display: block; word-wrap: break-word;
}
li {line-height: 1.25em;}
a:link {text-decoration: none; color: blue;}
a:visited {text-decoration: none; color: blue;}
.container {width: 750px; margin-left: auto; margin-right: auto; padding: 50px 0 50px 0;}
.title {text-align: left;}
.katex {font-size: 1em !important;} 
table {width: 100%%;}
table, th, td {border: 1px solid black;}

                </style>
                <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
                <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
                <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
                <script>
                    document.addEventListener("DOMContentLoaded", function() {
                        renderMathInElement(document.body, {
                            delimiters: [
                              {left: "$$", right: "$$", display: true},
                              {left: "$", right: "$", display: false}
                            ] 
                        });
                    });
                </script>
            </head>
            <body>
                <div class="container">
                    <h1 class="title">Support Vector Machine</h1>
                    2020-04-19
                    <span style="float:right; _position:relative;">
                        <a href="/home">HOME</a>
                        &ensp;
                        <a href="/notes">NOTES</a> &ensp; <a href="../">RELATED</a>
                    </span><br/>
                    <h2>NOTES 04/14</h2>
<p>Suppose we want to perform binary classification on N observations $y_i \in \left\{-1, 1 \right\}, i=1,\dots,N$ with $p$ features. Let us assume initially that the training set is separable.</p>
<p>Our goal is to define $\beta$ and $\beta_0$ and the classifier $G(\mathbf{x})=\mathrm{sgn}(\mathbf{x}^{\top}\beta+\beta_0)$. Let us define the half-margin $M$ and aim at</p>
<p>$$\underset{\beta, \beta_0}{\mathrm{max}} \space M \space s.t. \space \mathrm{dist}(\mathbf{x}_i, \mathrm{line}(\beta, \beta_0)) \geq M$$</p>
<p>But $\mathrm{dist}(\mathbf(x)_i, \mathrm{line}(\beta, \beta_0)) = \frac{|\mathbf{x}_i^{\top}\beta+\beta_0|}{||\beta||}=y_i \frac{(\mathbf{x}_i^{\top}\beta+\beta_0)}{||\beta||}$. And so we want</p>
<p>$$\underset{\beta, \beta_0}{\mathrm{max}} \space M \space s.t. \space y_i(\mathbf{x}_i^{\top}\beta+\beta_0) \geq M \cdot ||\beta||=1$$</p>
<p>Since the classifier $G(\mathbf{x})=\mathrm{sgn}(\mathbf{x}^{\top}\beta+\beta_0)$, $\beta$ and $\beta_0$ are non-identifiable up to a (positive) scale and so we can set $M \cdot ||\beta||=1 \Rightarrow M = \frac{1}{||\beta||}$. Thus,</p>
<p>$$\underset{\beta, \beta_0}{\mathrm{min}} ||\beta||\space s.t. \space y_i(\mathbf{x}_i^{\top}\beta+\beta_0) \geq 1$$</p>
<p>Note that this is a min-max problem: we want the largest minimum distance (&quot;margin&quot;). Now consider the more common non-separable case. We allow for points inside the margin boundary by adding &quot;slack&quot; variables $\xi_i \geq 0$, setting the relaxed condition</p>
<p>$$y_i \frac{(\mathbf{x}_i^{\top}\beta+\beta_0)}{||\beta||} \geq M(1-\xi_i), i=1,\dots,N$$</p>
<p>That is, $y_i (\mathbf{x}_i^{\top}\beta+\beta_0) \geq 1-\xi_i$.</p>
<p>If $\xi_i &gt; 0$, we allow a relative incursion into the boundary; in particular, if $\xi_i&gt;1$, the $i$-th observation is more than half-margin $M$ into the boundary, past the classifying plane, and so $i$ is misclassified. To avoid errors, we capture $\sum_{i=1}^{N}\xi_i \leq k$. The problem then is equivalent to</p>
<p>$$\underset{\beta, \beta_0}{\mathrm{min}} \space \frac{1}{2}||\beta||^2 + C \sum_{i=1}^{N}\xi_i$$</p>
<p>$$ s.t.\space \xi_i\geq 0, \space y_i(\mathbf{x}_i^{\top}\beta+\beta_0) \geq 1-\xi_i, \space i=1,\dots,N$$</p>
<p>With Lagrangian:</p>
<p>$$\mathscr{L}(\beta, \beta_0, \xi) = \frac{1}{2}||\beta||^2 + C \sum_{i=1}^{N}\xi_i-\sum_{i=1}^{N}\mu_i\xi_i - \sum_{i=1}^{N}\alpha_i \left[y_if(\mathbf{x}_i)-(1-\xi_i) \right]$$</p>
<p>where $\mu$ and $\alpha$ are KKT multipliers and we have $\mu_i \geq 0$ and $\alpha_i \geq 0$.</p>
<p>For stationarity, according to KKT conditions:</p>
<p>$$\begin{aligned} \frac{\partial \mathscr{L}}{\partial \beta}(\beta^{*}, \beta_0^{*}, \xi^{*})&amp;=\beta^{*}-\sum_{i}\alpha_iy_i\mathbf{x_i}=0 \\ &amp;\Rightarrow \beta^{*} = \mathbf{X}^{\top} \mathrm{Diag} \left\{y_i \right\} \cdot \alpha \\ \frac{\partial \mathscr{L}}{\partial \beta_0}(\beta^{*}, \beta_0^{*}, \xi^{*})&amp;= \alpha^{\top}\mathbf{y} = 0 \\ \frac{\partial \mathscr{L}}{\partial \xi}(\beta^{*}, \beta_0^{*}, \xi^{*})&amp;=C\mathbf{1}_N - \mu - \alpha \\ &amp;\Rightarrow \mu = C\mathbf{1}_N - \alpha \end{aligned}$$</p>
<p>Plugging back into $\mathscr{L}$, we get the &quot;dum&quot;</p>
<p>$$\mathscr{L}_D(\alpha)=\alpha^{\top}\mathbf{1}_N-\frac{1}{2}\alpha^{\top}\mathrm{D}_y \mathbf{X}\mathbf{X}^{\top}\mathrm{D}_y \alpha$$</p>
<p>with $\alpha^{\top}y=0$, $\mu_i \geq 0$ and $\alpha_i \leq C \Rightarrow 0 \leq \alpha_i \leq C$</p>
<p>The KKT &quot;complementary slackness&quot; conditions also impose:</p>
<p>$$\mu_i \xi_i = 0,\space \alpha_i\left[y_if(\mathbf{x}_i)-(1-\xi_i)\right]=0$$</p>
<p>After minimizing $\mathscr{L}_D$ and finding $\hat{\alpha}$, we have $\hat{\beta}=\mathbf{X}^T\mathrm{D}_y\hat{\alpha}=\sum_{i}\mathbf{x}_iy_i\hat{alpha}_i$ which only uses $i\space s.t.\space \hat{\alpha}_i&gt;0$ (&quot;support vectors&quot;).</p>
<p>If $\hat{\alpha}_i=C$, $\hat{\mu}_i=C-\hat{\alpha}_i \Rightarrow \xi_i \geq 0$;</p>
<p>If $0 &lt; \hat{\alpha}_i &lt; C$, $\hat{\mu}_i &gt; 0 \Rightarrow \xi_i = 0$ ($i$ in margin), so can be used to define $\hat{\beta}_0$</p>
<p>In general, we could have $f(\mathbf{X})=h(\mathbf{x})^{\top}\beta+\beta_0$. So, instead of $\mathbf{X} = \begin{bmatrix} \mathbf{x}_1^{\top} \\ \vdots \\ \mathbf{x}_N^{\top} \end{bmatrix}$ we have $\mathbf{H} = \begin{bmatrix} h(\mathbf{x}_1)^{\top} \\ \vdots \\ h(\mathbf{x}_N)^{\top} \end{bmatrix}$, which translates to the corresponding dual Lagrangian</p>
<p>$$\mathscr{L}_D(\alpha) = \alpha^{\top}\mathbf{1}_N - \frac{1}{2}\alpha^{\top}\mathrm{D}_y \mathbf{H}\mathbf{H}^{\top}\mathrm{D}_y$$</p>
<p>Here, instead of $\mathbf{X}\mathbf{X}^{\top}=[\mathbf{x}_i^{\top}\mathbf{x}_j]_{i,j=1,\dots,N}$,</p>
<p>we just have $\mathbf{H}\mathbf{H}^{\top}=[h(\mathbf{x}_i)^{\top}h(\mathbf{x}_j)]_{i,j}=\langle h(\mathbf{x}_i), h(\mathbf{x}_j) \rangle$.</p>
<p>Similarly, instead of $\hat{f}(\mathbf{x}_0)=\mathbf{x}_0^{\top}\hat{\beta}+\hat{\beta}_0$, we have</p>
<p>$$\begin{aligned} \hat{f}(\mathbf{x}_0) &amp;= h(\mathbf{x}_0)^{\top}\hat{\beta}+\hat{\beta}_0 \\ &amp;= h(\mathbf{x}_0)^{\top}\mathbf{H}^{\top} \mathrm{D}_y \hat{\alpha} + \hat{\beta} \\ &amp;= \sum_{i=1}^{N} \langle h(\mathbf{x}_0), h(\mathbf{x}_i) \rangle y_i \hat{\alpha}_i + \beta_0 \end{aligned}$$</p>
<p>It is clear then that we only need inner products, and so we can abstract and exploit the kernel trick: $\mathrm{K}(\mathbf{x}, \mathbf{z})=\langle h(\mathbf{x}), h(\mathbf{z})\rangle$ and specify $\mathrm{K}$ instead of the basis $h$.</p>
<p>E.G.,</p>
<ul>
<li>$\mathrm{K}(\mathbf{x}, \mathbf{z})=\left(1+\langle\mathbf{x}, \mathbf{z}\rangle \right)^d$, d-degree-polynomial</li>
<li>$\mathrm{K}(\mathbf{x}, \mathbf{z})=\mathrm{exp}\left\{-\gamma||\mathbf{x}-\mathbf{z}||^2\right\}$, &quot;radim&quot;</li>
<li>$\mathrm{K}(\mathbf{x}, \mathbf{z})=\mathrm{tanh}\left(\frac{\mathrm{K}}{2}\langle\mathbf{x}, \mathbf{z}\rangle+\frac{\mathrm{K}_0}{2}\right)$</li>
<li>$\mathrm{K}(\mathbf{x}, \mathbf{z})=2\sigma\left(\mathrm{K}\langle\mathbf{x}, \mathbf{z}\rangle+\mathrm{K}_0\right)-1$, &quot;neural network&quot;</li>
</ul>
<p>In fact, we can formally consider SVC in the context of regression; recall that with $f(\mathbf{x})=\mathbf{x}^{\top}\beta+\beta_0$, we had</p>
<p>$$\underset{\beta, \beta_0}{\mathrm{min}} \space \frac{1}{2}||\beta||^2 + C \sum_{i=1}^{N}\xi_i$$</p>
<p>$$ s.t.\space \xi_i\geq 0, \space y_i(\mathbf{x}_i^{\top}\beta+ \beta_0) \geq 1-\xi_i, \space i=1,\dots,N$$</p>
<p>$$y_if(\mathbf{x}_i)\leq 1-\xi_i \Rightarrow \xi_i \geq 1-y_if(\mathbf{x}_i) \Rightarrow \xi_i \geq [1-y_if(\mathbf{x}_i)]_{+}$$</p>
<p>Thus,</p>
<p>$$\begin{aligned}&amp;\underset{\beta, \beta_0}{\mathrm{min}} \space \frac{1}{2}||\beta||^2 + C \sum_{i=1}^{N}[1-y_if(\mathbf{x}_i)]_{+} \\ =&amp; \sum_{i=1}^{N}[1-y_if(\mathbf{x}_i)]_{+} + \frac{1}{2C}||\beta||^2 \end{aligned}$$</p>
<p>Back to the general setup,</p>
<p>$$\underset{\beta, \beta_0}{\mathrm{min}} \space \sum_{i=1}^{N}[1-y_if(\mathbf{x}_i)]_{+} + \frac{\lambda}{2}J_k(f)$$</p>
<p>We know the solution with basis representation $h_i(\mathbf{x})=\mathrm{K}(\mathbf{x},\mathbf{x}_i)$,  $f(\mathbf{x}) = h(\mathbf{x})^{\top}\hat{\beta}+\hat{\beta}_0$, $J_k(f)=\beta^{\top}[\mathrm{K}(\mathbf{x}_i, \mathbf{x}_j)]_{i,j}\beta = \beta^{\top}\mathrm{K}\beta$</p>
<p>E.G., $f(\mathbf{x})=\sum_{j=1}^{P}f_j(\mathbf{x}_j)$, additive cubic splines,</p>
<p>$$f_j(\mathbf{x})=\mathrm{N}_j(\mathbf{x})\theta_j$$</p>
<p>$$\lambda J(f)=\sum_j\lambda_j\theta_j^{\top}\Omega_j\theta_j = \sum_j\lambda_j\theta_j^{\top}\left[\int \mathrm{N}_{jk}^{\prime\prime}(t)\mathrm{N}_{jl}^{\prime\prime}(t) dt \right]_{k,l}\theta_j$$</p>

                    <p style="border-top: 1px solid #dbdbdb; margin-top: 40px;"></p>
                    <div id="valine-thread"></div>
                    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
                    <script src='/js/Valine.min.js'></script>
                    <script>
                        new Valine({
                            el: '#valine-thread',
                            appId: 'DSTtKETb5UeyXIjoPTpRnC8Y-gzGzoHsz',
                            appKey: 'csHaHWqxD2Ujv84O7jaJWOSc',
                            verify: false,
                            avatar: 'robohash',
                            placeholder: '',
                            meta: ['nick', 'mail'],
                            requiredFields: ['nick'],
                            visitor: true,
                            lang: 'en'
                        })
                    </script>
                  
                    <footer>
                        <p style="padding: 5px 0 0 0; text-align: center; font-size: .9rem; border-top: 1px solid #dbdbdb;margin-top: 40px;">&#169; 2021 Jimmy Z. Hu<br>This website is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0.</a></p>
                    </footer>
                </div>
            </body>
        </html>
    