
        <!DOCTYPE html>
        <html>
            <head>
                <meta http-equiv="Content-Type" content="text/html; charset=gb2312">
                <title>K-Nearest Neighbors and K-Dimensional Tree - Jimmy Z. Hu's Blog</title>
                <style rel="stylesheet" type="text/css">
                    body {font-family: Georgia; font-size: 17px;}
h2 {margin-top: 40px; border-bottom: 1px solid #999; text-transform: uppercase;}
p {text-align: left; line-height: 1.4em;}
pre {
    background: #f4f4f4; border: 1px solid #ddd; border-left: 3px solid #9f9f9f; 
    color: #666; page-break-inside: avoid; font-family: monospace; font-size: 15px; 
    line-height: 1.1; max-width: 100%; overflow: auto; padding: 0.5em 0.5em; 
    display: block; word-wrap: break-word;
}
li {line-height: 1.25em;}
a:link {text-decoration: none; color: blue;}
a:visited {text-decoration: none; color: blue;}
.container {width: 750px; margin-left: auto; margin-right: auto; padding: 50px 0 50px 0;}
.title {text-align: left;}
.katex {font-size: 1em !important;} 
table {width: 100%%;}
table, th, td {border: 1px solid black;}

                </style>
                <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
                <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
                <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
                <script>
                    document.addEventListener("DOMContentLoaded", function() {
                        renderMathInElement(document.body, {
                            delimiters: [
                              {left: "$$", right: "$$", display: true},
                              {left: "$", right: "$", display: false}
                            ] 
                        });
                    });
                </script>
            </head>
            <body>
                <div class="container">
                    <h1 class="title">K-Nearest Neighbors and K-Dimensional Tree</h1>
                    2020-01-08
                    <span style="float:right; _position:relative;">
                        <a href="/home">HOME</a>
                        &ensp;
                        <a href="/notes">NOTES</a> &ensp; <a href="../">RELATED</a>
                    </span><br/>
                    <p><a href="https://github.com/ZijingHu/Statistical-Learning-Methods-python-replication/blob/master/Chapter/Models/KNearestNeighbor.py">Code</a>  <a href="https://github.com/ZijingHu/Statistical-Learning-Methods-python-replication/blob/master/Chapter/Chapter-3-KNN.ipynb">Jupyter Notebook Demo</a></p>
<p>KNN is a simple but powerful supervised machine learning algorithm that can be used for both classification and regression problems. The assumption for kNN is that we already have a training set with certain features and labels. When giving a new instance, the model will search its k nearest neighbors in the training set and use majority voting to pick the class for prediction or regression.</p>
<p>Given $x_{i}=(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)})^{\mathrm{T}}$, $x_{j}=(x_{j}^{(1)}, x_{j}^{(2)}, \cdots, x_{j}^{(n)})^{\mathrm{T}}$, s.t. $x_i$, $x_j \in X = R^n$, the distance between $x_i$ and $x_j$ is defined by:$$L_{p}(x_{i}, x_{j})=(\sum_{l=1}^{n}|x_{i}^{(l)}-x_{j}^{(l)}|^{p})^{\frac{1}{p}}$$Usually, the Euclidean distance ($p=2$) is used as the distance metric.</p>
<h2>Experiments with Different Parameters</h2>
<br>
<table>
<tr><th width=30% style="text-align:center">Neighbors</th><th widht=5% style="text-align:center">Norm</th><th width=30% style="text-align:center">Training Score</th><th widht=20% style="text-align:center">Testing Score</th></tr>
<tr><td><center>1</center></td><td><center>2</center></td><td><center><b>100%</center></td><td><center>77.4%</center></td></tr>
<tr><td><center>3</center></td><td><center>2</center></td><td><center>88.4%</center></td><td><center>78.5%</center></td></tr>
<tr><td><center>3</center></td><td><center>1</center></td><td><center>87.6%</center></td><td><center>77.2%</center></td></tr>
<tr><td><center>8</center></td><td><center>2</center></td><td><center>85.9%</center></td><td><center>81.8%</center></td></tr>
<tr><td><center>8</center></td><td><center>1</center></td><td><center>86.1%</center></td><td><center>81.6%</center></td></tr>
<tr><td><center>15</center></td><td><center>2</center></td><td><center>86.3%</center></td><td><center><b>83.9%</center></td></tr>
</table>
<br>
<p>When setting $k=1$, the model uses the nearest point to predict. Thus, as shown in the graph below, the performance on training set reaches an accuracy of 100% since every point is predicted by itself. However, this model will easily fit noise or outliers and result in overfitting.
<br><center><img src="n1d2.svg" width=100%></center><br></p>
<p>Increasing k to 3 allows the model to ignore some unreasonable points and generate a better decision boundary. The graphs below are results of models with $l=1$ and $l=2$. The shapes of decision boundary of two models are very similar to each other.</p>
<p><br><center><img src="n3d1.svg" width=100%></center></p>
<center><img src="n3d2.svg" width=100%></center><br>
<p>When choosing a larger k, more points are involved in the voting process, which makes the decision boundary smoother and less likely to overfitting.</p>
<p><br><center><img src="n15d2.svg" width=100%></center><br></p>
<p>However, allowing too many points to enter the voting process may cause other problems such underfitting.</p>
<p><br><center><img src="score.svg" width=100%></center><br></p>
<h2>K-Dimensional Tree (k-d tree)</h2>
<p>K-d tree is a space-partitioning data structure for organizing points in a k-dimensional space. Using k-d tree to search nearest neighbors only has $O(N)$ time complexity for each new instance where $N$ = Number of records in training set. Otherwise, the time complexity at least would be $O(N^2 log N)$ if directly calculate distances from new instances to every points in training set</p>
<h3>Construction of K-Dimensional Tree</h3>
<p>Since there are many possible ways to choose axis-aligned splitting planes, there are many different ways to construct k-d trees. The canonical method of k-d tree construction has the following constraints:</p>
<ul>
<li>As one moves down the tree, one cycles through the axes used to select the splitting planes. (For example, in a 3-dimensional tree, the root would have an x-aligned plane, the root's children would both have y-aligned planes, the root's grandchildren would all have z-aligned planes, the root's great-grandchildren would all have x-aligned planes, the root's great-great-grandchildren would all have y-aligned planes, and so on.) The axis $l$ can be calculated by $j(mod$ $k)+1$, where $k$ is the number of axes and j is the depth of the tree.</li>
<li>Points are inserted by selecting the median of the points base on the selected axis in each cycle. On every node, the median of the points will be assigned to the node, and points on the left side of the median will be assigned to the left sub-tree while the right side will be assigned to the right sub-tree.</li>
</ul>
<p>This method leads to a balanced k-d tree, in which each leaf node is approximately the same distance from the root. However, balanced trees are not necessarily optimal for all applications.</p>
<p><br><center><img src="kdtree.svg" width=100%></center><br></p>
<h3>Nearest Neighbour Search</h3>
<p>Searching for a nearest neighbour in a k-d tree proceeds as follows:</p>
<ol>
<li>Starting with the root node, the algorithm moves down the tree recursively until it reaches a leaf, and save the smallest distance between the new point and every passed node as &quot;current best&quot;. Going left or right depending on whether the point is lesser than or greater than the current node in the split dimension.</li>
<li>Check the splitting plane of the parent node. If the distance is better, move to the sibling tree and repeat 1.</li>
</ol>
<p><br><center><img src="KDTree-animation.gif" width=100%></center><br></p>
<h2>Reference</h2>
<p>[1] Li, Hang. (2019). Statistical Learning Methods, Second Edition. Tsinghua University Press.<br />
[2] Samet, H. (1990). The design and analysis of spatial data structures (Vol. 85, p. 77). Reading, MA: Addison-Wesley.<br />
[3] Wikipedia contributors. (2019, December 22). K-d tree. In Wikipedia, The Free Encyclopedia. Retrieved 20:01, January 8, 2020, from https://en.wikipedia.org/w/index.php?title=K-d\tree&amp;oldid=931917768
<br></p>

                    <p style="border-top: 1px solid #dbdbdb; margin-top: 40px;"></p>
                    <div id="valine-thread"></div>
                    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
                    <script src='/js/Valine.min.js'></script>
                    <script>
                        new Valine({
                            el: '#valine-thread',
                            appId: 'DSTtKETb5UeyXIjoPTpRnC8Y-gzGzoHsz',
                            appKey: 'csHaHWqxD2Ujv84O7jaJWOSc',
                            verify: false,
                            avatar: 'robohash',
                            placeholder: '',
                            meta: ['nick', 'mail'],
                            requiredFields: ['nick'],
                            visitor: true,
                            lang: 'en'
                        })
                    </script>
                  
                    <footer>
                        <p style="padding: 5px 0 0 0; text-align: center; font-size: .9rem; border-top: 1px solid #dbdbdb;margin-top: 40px;">&#169; 2021 Jimmy Z. Hu<br>This website is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0.</a></p>
                    </footer>
                </div>
            </body>
        </html>
    