
        <!DOCTYPE html>
        <html>
            <head>
                <meta http-equiv="Content-Type" content="text/html; charset=gb2312">
                <title>Three Types of Naive Bayes Classifier - Jimmy Z. Hu's Blog</title>
                <style rel="stylesheet" type="text/css">
                    body {font-family: Georgia; font-size: 17px;}
h2 {margin-top: 40px; border-bottom: 1px solid #999; text-transform: uppercase;}
p {text-align: left; line-height: 1.4em;}
pre {
    background: #f4f4f4; border: 1px solid #ddd; border-left: 3px solid #9f9f9f; 
    color: #666; page-break-inside: avoid; font-family: monospace; font-size: 15px; 
    line-height: 1.1; max-width: 100%; overflow: auto; padding: 0.5em 0.5em; 
    display: block; word-wrap: break-word;
}
li {line-height: 1.25em;}
a:link {text-decoration: none; color: blue;}
a:visited {text-decoration: none; color: blue;}
.container {width: 750px; margin-left: auto; margin-right: auto; padding: 50px 0 50px 0;}
.title {text-align: left;}
.katex {font-size: 1em !important;} 
table {width: 100%%;}
table, th, td {border: 1px solid black;}

                </style>
                <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
                <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
                <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
                <script>
                    document.addEventListener("DOMContentLoaded", function() {
                        renderMathInElement(document.body, {
                            delimiters: [
                              {left: "$$", right: "$$", display: true},
                              {left: "$", right: "$", display: false}
                            ] 
                        });
                    });
                </script>
            </head>
            <body>
                <div class="container">
                    <h1 class="title">Three Types of Naive Bayes Classifier</h1>
                    2020-01-19
                    <span style="float:right; _position:relative;">
                        <a href="/home">HOME</a>
                        &ensp;
                        <a href="/notes">NOTES</a> &ensp; <a href="../">RELATED</a>
                    </span><br/>
                    <p><a href="https://github.com/ZijingHu/Statistical-Learning-Methods-python-replication/blob/master/Chapter/Models/NaiveBayesianClassifier.py">Code</a>  <a href="https://github.com/ZijingHu/Statistical-Learning-Methods-python-replication/blob/master/Chapter/Chapter-4-NaiveBayes.ipynb">Jupyter Notebook Demo</a></p>
<p>Bernoulli Naive Bayes, Gaussian Naive Bayes and Multinomial Naive Bayes are three types of navie bayes model that are included in scikit-learn library. They are all derived from Bayes theorem but have different assumptions of how features distribute. This article will explain the basics of these algorithms and use some experiments to show how they work.</p>
<h3>Bayes Theorem and Naive Bayes Classifier</h3>
<p>Bayes theorem provides a way of calculating posterior probability $p(C_{k} | \mathbf{x})$ with the formula $p(C_{k} | \mathbf{x})=\frac{p(C_{k}) p(\mathbf{x} | C_{k})}{p(\mathbf{x})}$, where $p(C_{k})$ represents the piror probability, $p(\mathbf{x} | C_{k})$ represents the likelihood function and $p(\mathbf{x})$ is the probability of event $\mathbf{x}$ occurs. In &quot;Naive&quot; Bayes Model, we &quot;naively&quot; assume that all features in ${\displaystyle \mathbf {x}}$ are mutually independent. Thus, $p(\mathbf{x} | C_{k})$ can be rewriten as $\prod_{i=1}^{n} p(x_{i} | C_{k})$ and the posterior probability can be expressed as</p>
<p>$$\begin{aligned} p(C_{k} | x_{1}, \ldots, x_{n}) &amp; = \frac{p(C_{k}) \prod_{i=1}^{n} p(x_{i} | C_{k})}{p(\mathbf{x})} \\ &amp; \propto p(C_{k}) \prod_{i=1}^{n} p(x_{i} | C_{k}) \end{aligned}$$</p>
<p>Naive Bayes classifier follows the maximum a posteriori or MAP decision rule and classifies any newly input observation as $\hat{y}=\underset{C_k}{\operatorname{argmax}} \space p(C_{k}) \prod_{i=1}^{n} p(x_{i} | C_{k})$. The following part of this article will introduce three most popular naive bayes classifers and their applications.</p>
<h3>Bernoulli Naive Bayes</h3>
<p>Bernoulli Naive Bayes assumes that all features are binary distributed. Thus, this model only takes categorical data as input since every categorical feature can be replaced by $n-1$ binary variables, where $n$ is the number of category in the corresponding feature. According to the probability mass function of Bernoulli distribution, $f(k ; p)=p^{k}(1-p)^{1-k} \space for \space k \in{0,1}$, the likelihood can be expressed as $p(\mathbf{x} | C_{k})=\prod_{i=1}^{n} p_{k i}^{x_{i}}(1-p_{k i})^{(1-x_{i})}$. Therefore, the posterior probability will be calculated by $$ p(C_{k} | x_{1}, \ldots, x_{n}) \propto p(C_{k}) \prod_{i=1}^{n} p_{k i}^{x_{i}}(1-p_{k i})^{(1-x_{i})} $$</p>
<p>Here I used randomly generated data to show how this classifier works. The boundary on the graphs is not the true dicision boundary since the model only accepts categorial data. Actually, the classification cannot be visualized by a clear decision boundary on the graph. So, I rounded all points to its closest category to make every points on the graph become valid inputs for the classifer and drew a &quot;decision boundary&quot; based on these processed inputs.</p>
<p><br><center><img src="BNB.svg" width=100%></center></p>
<h3>Gaussian Naive Bayes</h3>
<p>In Bernoulli Naive Bayes, all features need to be categorical, and all posibile values in every feature have to exist in training set before making any prediction. In most cases, however, dataset has mixed types of featuret, which completely violates the reistriction of Bernoulli Naive Bayes. To solve this issue, we turn to Gaussian distribution, a more powerful distribution. Gaussian Naive Bayes assumes that all features follow Gaussian distribution, $p(x)=\frac{1}{\sqrt{2 \pi \sigma_{k}^{2}}} e^{-\frac{(x -\mu_{k})^{2}}{2 \sigma_{k}^{2}}}$. Thus, the posterior probability can be expressed as $$p(C_{k} | x_{1}, \ldots, x_{n}) \propto p(C_{k}) \prod_{i=1}^{n}\frac{1}{\sqrt{2 \pi \sigma_{k}^{2}}} e^{-\frac{(x_i-\mu_{k})^{2}}{2 \sigma_{k}^{2}}}$$</p>
<p>Since Gaussian distribution works for continious data, the decision boundary of Gaussian Naive Bayes can be perfectly visualized on the graph. As shown on the graphs, Gaussian Naive Bayes's classification boundary is smooth and flexiable, making it more powerful to deal with data with mix types of feature.</p>
<p><br><center><img src="GNB.svg" width=100%></center></p>
<h3>Multinomial Naive Bayes</h3>
<p>The decision boundary of Multinomial Naive Bayes is hard to be visualized since its input features are usually represent counts or count rates. This classifier  assumes all features follow a multinomial distribution $p(\mathbf{x})=\frac{(\sum_{i} x_{i}) !}{\prod_{i} x_{i} !} \prod_{i=1}^{n} p_{i}^{x_i}$ s.t. $\sum_{i}^n p_{i}=1$, where $p_i$ represents the probability of event $i$ occurs. Thus, the likelihood is $p(\mathbf{x} | C_{k})=\frac{(\sum_{i} x_{i}) !}{\prod_{i} x_{i} !} \prod_{i=1}^{n} p_{k i}^{x_i}$, and the posterior probability is expressed as $$ p(C_{k} | x_{1}, \ldots, x_{n}) \propto p(C_{k}) \frac{(\sum_{i} x_{i}) !}{\prod_{i} x_{i} !} \prod_{i=1}^{n} p_{i}^{x_i}$$</p>
<p>Multinomial Naive Bayes is a very useful tool for text classification since every inputing features can be used to represents frequencies of a certain word in a sentence. Thus, here I used <a href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews">IMDB movie reviews dataset</a> from Kaggle to show how this model works,.</p>
<p>The dataset includes 50k movie reviews and their sentiment labels.</p>
<pre class='hljs' style=white-space:pre-wrap;word-wrap:break-word>
<code>>> print(df.review.values[1])

A wonderful little production. 

The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. 

The actors are extremely well chosen- Michael Sheen not only "has got all the polari" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. 

The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.

>> print(df.sentiment.values[1])

'positive'</code></pre>
<p>I stripped all meaningless punctuations and use a <a href="https://github.com/ZijingHu/Statistical-Learning-Methods-python-replication/blob/master/Chapter/Dataset/word_dict.csv">prepared word dictionary</a> to transform reviews into vectors. Every position in the vector represents a word from the word dictionary and every element is the count of the corresponding word. These vectors will be used as the input for the classifier. More details about text processing can be found <a href="https://github.com/ZijingHu/Statistical-Learning-Methods-python-replication/tree/master/Chapter/Tools/text_processing.py">here</a>.</p>
<p>Below is the core part of the code and testing result.</p>
<pre class='hljs' style=white-space:pre-wrap;word-wrap:break-word>  
<code>>> # www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews  
>> df = pd.read_csv('imdb_dataset.csv', error_bad_lines=False, engine='python')  
>>  
>> # import a prepared a dictionary  
>> word_list_ext = pd.read_csv('word_dict.csv')['word'].values  
>> word_dict_ext = dict(zip(set(word_list_ext), range(len(set(word_list_ext)))))  
>> word_index = word_dict(df.review, len(word_dict_ext))  
>>  
>> X = text2array(df.review, word_dict_ext)  
>> y = (df.sentiment.values=='positive')*1.0  
>> X_train, X_test, y_train, y_test = split_train_test(X, y, 0.2)  
>>  
>> mnb = Multinomial()  
>> mnb.fit(X_train, y_train)  
>> print('Training score: %s\nTesting score: %s' %   
         (mnb.score(X_train, y_train), mnb.score(X_test, y_test)))  

Training score: 0.8346  
Testing score: 0.8022</code></pre>  
<p>I used a little trick in above experiment - a prepared word dictionary. Before using this, I tried used the top X (X can be 500, 1000, 2000...) most frequent words in the training set to construct a word dictionary for generating vectors. However, the model performances were so terrible with these dictionary. So, I turned to a special dictionary, where the words are related more closely with sentiments, and finally resulted in a much better predicting score. Building such dictionary is not very hard. Just need to find those words that have the largest $|p(positive|word)-p(negitive|word)|$.
<br></p>
<h2>Reference</h2>
<p>[1] Li, Hang. (2019). Statistical Learning Methods, Second Edition. Tsinghua University Press.<br />
[2] Raschka, S. (2014). Naive bayes and text classification i-introduction and theory. arXiv preprint arXiv:1410.5329.<br />
[3] Raschka, S. Naive Bayes and Text Classification. (2014, October 4). Retrieved January 20, 2020, from https://sebastianraschka.com/Articles/2014_naive_bayes_1.html<br />
[4] Wikipedia contributors. (2020, January 14). Naive Bayes classifier. In Wikipedia, The Free Encyclopedia. Retrieved 07:05, January 20, 2020, from https://en.wikipedia.org/w/index.php?title=Naive_Bayes_classifier&amp;oldid=935822105
<br></p>

                    <p style="border-top: 1px solid #dbdbdb; margin-top: 40px;"></p>
                    <div id="valine-thread"></div>
                    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
                    <script src='/js/Valine.min.js'></script>
                    <script>
                        new Valine({
                            el: '#valine-thread',
                            appId: 'DSTtKETb5UeyXIjoPTpRnC8Y-gzGzoHsz',
                            appKey: 'csHaHWqxD2Ujv84O7jaJWOSc',
                            verify: false,
                            avatar: 'robohash',
                            placeholder: '',
                            meta: ['nick', 'mail'],
                            requiredFields: ['nick'],
                            visitor: true,
                            lang: 'en'
                        })
                    </script>
                  
                    <footer>
                        <p style="padding: 5px 0 0 0; text-align: center; font-size: .9rem; border-top: 1px solid #dbdbdb;margin-top: 40px;">&#169; 2021 Jimmy Z. Hu<br>This website is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0.</a></p>
                    </footer>
                </div>
            </body>
        </html>
    